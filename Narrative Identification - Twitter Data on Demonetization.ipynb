{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = \"demonetization\" \n",
    "file_name = \"demonetization-tweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os \n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "import chardet\n",
    "from textblob import TextBlob\n",
    "from gensim.models import word2vec\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the encoding of the data file\n",
    "with open('./demonetization-tweets.csv','rb') as f:\n",
    "    result = chardet.detect(f.read())  #Windows-1252\n",
    "\n",
    "# Import the data file\n",
    "df = pd.read_csv(\"./\"+file_name+\".csv\", encoding=result['encoding'])\n",
    "#df = df[1:1000+1]\n",
    "df = df['text']\n",
    "df = pd.DataFrame({'tweet':df})    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Clean the tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravkantkumar/Library/Python/3.9/lib/python/site-packages/pandas/core/dtypes/inference.py:182: FutureWarning: Possible nested set at position 1\n",
      "  re.compile(obj)\n"
     ]
    }
   ],
   "source": [
    "# Clean the tweets    \n",
    "df['cleaned_tweet'] = df['tweet'].replace(r'\\'|\\\"|\\,|\\.|\\?|\\+|\\-|\\/|\\=|\\(|\\)|\\n|\"', '', regex=True)\n",
    "df['cleaned_tweet'] = df['cleaned_tweet'].replace(\"  \", \" \")\n",
    "\n",
    "words_remove = [\"ax\",\"i\",\"you\",\"edu\",\"s\",\"t\",\"m\",\"subject\",\"can\",\"lines\",\"re\",\"what\", \"there\",\"all\",\"we\",\n",
    "                \"one\",\"the\",\"a\",\"an\",\"of\",\"or\",\"in\",\"for\",\"by\",\"on\",\"but\",\"is\",\"in\",\"a\",\"not\",\"with\",\"as\",\n",
    "                \"was\",\"if\",\"they\",\"are\",\"this\",\"and\",\"it\",\"have\",\"has\",\"from\",\"at\",\"my\",\"be\",\"by\",\"not\",\"that\",\"to\",\n",
    "                \"from\",\"com\",\"org\",\"like\",\"likes\",\"so\",\"said\",\"from\",\"what\",\"told\",\"over\",\"more\",\"other\",\n",
    "                \"have\",\"last\",\"with\",\"this\",\"that\",\"such\",\"when\",\"been\",\"says\",\"will\",\"also\",\"where\",\"why\",\n",
    "                \"would\",\"today\", \"in\", \"on\", \"you\", \"r\", \"d\", \"u\", \"hw\",\"wat\", \"oly\", \"s\", \"b\", \"ht\", \n",
    "                \"rt\", \"p\",\"the\",\"th\", \"n\", \"was\"]\n",
    "\n",
    "\n",
    "def cleantext(df, words_to_remove = words_remove): \n",
    "    ### dont change the original tweet\n",
    "    # remove emoticons form the tweets\n",
    "    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'<ed>','', regex = True)\n",
    "    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'\\B<U+.*>|<U+.*>\\B|<U+.*>','', regex = True)\n",
    "    \n",
    "    # convert tweets to lowercase\n",
    "    df['cleaned_tweet'] = df['cleaned_tweet'].str.lower()\n",
    "    \n",
    "    #remove user mentions\n",
    "    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'^(@\\w+)',\"\", regex=True)\n",
    "    \n",
    "    #remove 'rt' in the beginning\n",
    "    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'^(rt @)',\"\", regex=True)\n",
    "    \n",
    "    #remove_symbols\n",
    "    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'[^a-zA-Z0-9]', \" \", regex=True)\n",
    "\n",
    "    #remove punctuations \n",
    "    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'[[]!\"#$%\\'()\\*+,-./:;<=>?^_`{|}]+',\"\", regex = True)\n",
    "\n",
    "    #remove_URL(x):\n",
    "    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'https.*$', \"\", regex = True)\n",
    "\n",
    "    #remove 'amp' in the text\n",
    "    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'amp',\"\", regex = True)\n",
    "    \n",
    "    #remove words of length 1 or 2 \n",
    "    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'\\b[a-zA-Z]{1,2}\\b','', regex=True)\n",
    "\n",
    "    #remove extra spaces in the tweet\n",
    "    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'^\\s+|\\s+$',\" \", regex=True)\n",
    "     \n",
    "    \n",
    "    #remove stopwords and words_to_remove\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    mystopwords = [stop_words, \"via\", words_to_remove]\n",
    "    \n",
    "    df['fully_cleaned_tweet'] = df['cleaned_tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in mystopwords]))\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "#get the processed tweets\n",
    "df = cleantext(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis\n",
    "\n",
    "df['sentiment'] = df['fully_cleaned_tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)  #-1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>fully_cleaned_tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @rssurjewala: Critical question: Was PayTM ...</td>\n",
       "      <td>rssurjewala  critical question  was paytm info...</td>\n",
       "      <td>rssurjewala critical question was paytm inform...</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Hemant_80: Did you vote on #Demonetization...</td>\n",
       "      <td>hemant 80  did you vote   demonetization  modi...</td>\n",
       "      <td>hemant 80 did you vote demonetization modi sur...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @roshankar: Former FinSec, RBI Dy Governor,...</td>\n",
       "      <td>roshankar  former finsec rbi  governor cbdt ch...</td>\n",
       "      <td>roshankar former finsec rbi governor cbdt chai...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @ANI_news: Gurugram (Haryana): Post office ...</td>\n",
       "      <td>ani news  gurugram haryana  post office employ...</td>\n",
       "      <td>ani news gurugram haryana post office employee...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @satishacharya: Reddy Wedding! @mail_today ...</td>\n",
       "      <td>satishacharya  reddy wedding   mail today cart...</td>\n",
       "      <td>satishacharya reddy wedding mail today cartoon...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet   \n",
       "0  RT @rssurjewala: Critical question: Was PayTM ...  \\\n",
       "1  RT @Hemant_80: Did you vote on #Demonetization...   \n",
       "2  RT @roshankar: Former FinSec, RBI Dy Governor,...   \n",
       "3  RT @ANI_news: Gurugram (Haryana): Post office ...   \n",
       "4  RT @satishacharya: Reddy Wedding! @mail_today ...   \n",
       "\n",
       "                                       cleaned_tweet   \n",
       "0  rssurjewala  critical question  was paytm info...  \\\n",
       "1  hemant 80  did you vote   demonetization  modi...   \n",
       "2  roshankar  former finsec rbi  governor cbdt ch...   \n",
       "3  ani news  gurugram haryana  post office employ...   \n",
       "4  satishacharya  reddy wedding   mail today cart...   \n",
       "\n",
       "                                 fully_cleaned_tweet  sentiment  \n",
       "0  rssurjewala critical question was paytm inform...       0.15  \n",
       "1  hemant 80 did you vote demonetization modi sur...       0.00  \n",
       "2  roshankar former finsec rbi governor cbdt chai...       0.00  \n",
       "3  ani news gurugram haryana post office employee...       0.00  \n",
       "4  satishacharya reddy wedding mail today cartoon...       0.00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Vectorize the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_tweet'] = df['fully_cleaned_tweet'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>fully_cleaned_tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @rssurjewala: Critical question: Was PayTM ...</td>\n",
       "      <td>rssurjewala  critical question  was paytm info...</td>\n",
       "      <td>rssurjewala critical question was paytm inform...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>[rssurjewala, critical, question, was, paytm, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Hemant_80: Did you vote on #Demonetization...</td>\n",
       "      <td>hemant 80  did you vote   demonetization  modi...</td>\n",
       "      <td>hemant 80 did you vote demonetization modi sur...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[hemant, 80, did, you, vote, demonetization, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @roshankar: Former FinSec, RBI Dy Governor,...</td>\n",
       "      <td>roshankar  former finsec rbi  governor cbdt ch...</td>\n",
       "      <td>roshankar former finsec rbi governor cbdt chai...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[roshankar, former, finsec, rbi, governor, cbd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @ANI_news: Gurugram (Haryana): Post office ...</td>\n",
       "      <td>ani news  gurugram haryana  post office employ...</td>\n",
       "      <td>ani news gurugram haryana post office employee...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[ani, news, gurugram, haryana, post, office, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @satishacharya: Reddy Wedding! @mail_today ...</td>\n",
       "      <td>satishacharya  reddy wedding   mail today cart...</td>\n",
       "      <td>satishacharya reddy wedding mail today cartoon...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[satishacharya, reddy, wedding, mail, today, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14935</th>\n",
       "      <td>RT @saxenavishakha: Ghost of demonetization re...</td>\n",
       "      <td>saxenavishakha  ghost  demonetization returns ...</td>\n",
       "      <td>saxenavishakha ghost demonetization returns wi...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[saxenavishakha, ghost, demonetization, return...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14936</th>\n",
       "      <td>N d modi fans-d true nationalists of the count...</td>\n",
       "      <td>modi fansd true nationalists  the country sti...</td>\n",
       "      <td>modi fansd true nationalists the country stil ...</td>\n",
       "      <td>-0.105556</td>\n",
       "      <td>[modi, fansd, true, nationalists, the, country...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14937</th>\n",
       "      <td>RT @bharat_builder: Lol. Demonetization has fi...</td>\n",
       "      <td>bharat builder  lol demonetization has fixed  ...</td>\n",
       "      <td>bharat builder lol demonetization has fixed lo...</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>[bharat, builder, lol, demonetization, has, fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14938</th>\n",
       "      <td>RT @Stupidosaur: @Vidyut B team of BJP. CIA ba...</td>\n",
       "      <td>stupidosaur   vidyut  team  bjp cia baby cctv ...</td>\n",
       "      <td>stupidosaur vidyut team bjp cia baby cctv evm ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[stupidosaur, vidyut, team, bjp, cia, baby, cc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14939</th>\n",
       "      <td>@Vidyut B team of BJP. CIA baby. CCTV, EVM but...</td>\n",
       "      <td>team  bjp cia baby cctv evm but with vvpat su...</td>\n",
       "      <td>team bjp cia baby cctv evm but with vvpat supp...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[team, bjp, cia, baby, cctv, evm, but, with, v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14940 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet   \n",
       "0      RT @rssurjewala: Critical question: Was PayTM ...  \\\n",
       "1      RT @Hemant_80: Did you vote on #Demonetization...   \n",
       "2      RT @roshankar: Former FinSec, RBI Dy Governor,...   \n",
       "3      RT @ANI_news: Gurugram (Haryana): Post office ...   \n",
       "4      RT @satishacharya: Reddy Wedding! @mail_today ...   \n",
       "...                                                  ...   \n",
       "14935  RT @saxenavishakha: Ghost of demonetization re...   \n",
       "14936  N d modi fans-d true nationalists of the count...   \n",
       "14937  RT @bharat_builder: Lol. Demonetization has fi...   \n",
       "14938  RT @Stupidosaur: @Vidyut B team of BJP. CIA ba...   \n",
       "14939  @Vidyut B team of BJP. CIA baby. CCTV, EVM but...   \n",
       "\n",
       "                                           cleaned_tweet   \n",
       "0      rssurjewala  critical question  was paytm info...  \\\n",
       "1      hemant 80  did you vote   demonetization  modi...   \n",
       "2      roshankar  former finsec rbi  governor cbdt ch...   \n",
       "3      ani news  gurugram haryana  post office employ...   \n",
       "4      satishacharya  reddy wedding   mail today cart...   \n",
       "...                                                  ...   \n",
       "14935  saxenavishakha  ghost  demonetization returns ...   \n",
       "14936   modi fansd true nationalists  the country sti...   \n",
       "14937  bharat builder  lol demonetization has fixed  ...   \n",
       "14938  stupidosaur   vidyut  team  bjp cia baby cctv ...   \n",
       "14939   team  bjp cia baby cctv evm but with vvpat su...   \n",
       "\n",
       "                                     fully_cleaned_tweet  sentiment   \n",
       "0      rssurjewala critical question was paytm inform...   0.150000  \\\n",
       "1      hemant 80 did you vote demonetization modi sur...   0.000000   \n",
       "2      roshankar former finsec rbi governor cbdt chai...   0.000000   \n",
       "3      ani news gurugram haryana post office employee...   0.000000   \n",
       "4      satishacharya reddy wedding mail today cartoon...   0.000000   \n",
       "...                                                  ...        ...   \n",
       "14935  saxenavishakha ghost demonetization returns wi...   0.000000   \n",
       "14936  modi fansd true nationalists the country stil ...  -0.105556   \n",
       "14937  bharat builder lol demonetization has fixed lo...   0.183333   \n",
       "14938  stupidosaur vidyut team bjp cia baby cctv evm ...   0.000000   \n",
       "14939  team bjp cia baby cctv evm but with vvpat supp...   0.000000   \n",
       "\n",
       "                                         tokenized_tweet  \n",
       "0      [rssurjewala, critical, question, was, paytm, ...  \n",
       "1      [hemant, 80, did, you, vote, demonetization, m...  \n",
       "2      [roshankar, former, finsec, rbi, governor, cbd...  \n",
       "3      [ani, news, gurugram, haryana, post, office, e...  \n",
       "4      [satishacharya, reddy, wedding, mail, today, c...  \n",
       "...                                                  ...  \n",
       "14935  [saxenavishakha, ghost, demonetization, return...  \n",
       "14936  [modi, fansd, true, nationalists, the, country...  \n",
       "14937  [bharat, builder, lol, demonetization, has, fi...  \n",
       "14938  [stupidosaur, vidyut, team, bjp, cia, baby, cc...  \n",
       "14939  [team, bjp, cia, baby, cctv, evm, but, with, v...  \n",
       "\n",
       "[14940 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if a word has a digit, remove that word\n",
    "df['tokenized_tweet'] = df['tokenized_tweet'].apply(lambda x: [y for y in x if not any(c.isdigit() for c in y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 100    # Word vector dimensionality                      \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model (this will take some time)\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(df['tokenized_tweet'], workers=num_workers, \\\n",
    "            vector_size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find vector corresponding to each tweet\n",
    "Take the average of all word vectors in a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['demonetization',\n",
       " 'the',\n",
       " 'india',\n",
       " 'and',\n",
       " 'modi',\n",
       " 'that',\n",
       " 'out',\n",
       " 'for',\n",
       " 'who',\n",
       " 'had',\n",
       " 'narendra',\n",
       " 'rich',\n",
       " 'after',\n",
       " 'find',\n",
       " 'dear',\n",
       " 'implement',\n",
       " 'evanspiegel',\n",
       " 'actually',\n",
       " 'from',\n",
       " 'have',\n",
       " 'urautelaforever',\n",
       " 'was',\n",
       " 'people',\n",
       " 'narendramodi',\n",
       " 'bank',\n",
       " 'will',\n",
       " 'has',\n",
       " 'this',\n",
       " 'how',\n",
       " 'are',\n",
       " 'with',\n",
       " 'you',\n",
       " 'about',\n",
       " 'cash',\n",
       " 'impact',\n",
       " 'lakh',\n",
       " 'support',\n",
       " 'its',\n",
       " 'such',\n",
       " 'terrorists',\n",
       " 'all',\n",
       " 'over',\n",
       " 'against',\n",
       " 'not',\n",
       " 'nation',\n",
       " 'since',\n",
       " 'thats',\n",
       " 'they',\n",
       " 'move',\n",
       " 'third',\n",
       " 'incident',\n",
       " 'looted',\n",
       " 'modibharosa',\n",
       " 'kishtwar',\n",
       " 'gauravcsawant',\n",
       " 'youtube',\n",
       " 'why',\n",
       " 'money',\n",
       " 'question',\n",
       " 'back',\n",
       " 'but',\n",
       " 'across',\n",
       " 'atms',\n",
       " 'like',\n",
       " 'now',\n",
       " 'more',\n",
       " 'his',\n",
       " 'due',\n",
       " 'says',\n",
       " 'supports',\n",
       " 'goes',\n",
       " 'paytm',\n",
       " 'our',\n",
       " 'drkumarvishwas',\n",
       " 'oscar',\n",
       " 'should',\n",
       " 'app',\n",
       " 'govt',\n",
       " 'whether',\n",
       " 'good',\n",
       " 'still',\n",
       " 'modis',\n",
       " 'bjp',\n",
       " 'can',\n",
       " 'notes',\n",
       " 'full',\n",
       " 'when',\n",
       " 'shortage',\n",
       " 'poor',\n",
       " 'party',\n",
       " 'clearly',\n",
       " 'your',\n",
       " 'what',\n",
       " 'critical',\n",
       " 'rssurjewala',\n",
       " 'bypolls',\n",
       " 'informed',\n",
       " 'edict',\n",
       " 'first',\n",
       " 'again',\n",
       " 'fishy',\n",
       " 'requires',\n",
       " 'politics',\n",
       " 'effect',\n",
       " 'rahulroushan',\n",
       " 'huge',\n",
       " 'before',\n",
       " 'disclosure',\n",
       " 'cvoter',\n",
       " 'him',\n",
       " 'were',\n",
       " 'today',\n",
       " 'man',\n",
       " 'any',\n",
       " 'black',\n",
       " 'putting',\n",
       " 'nitishkumar',\n",
       " 'economy',\n",
       " 'she',\n",
       " 'dont',\n",
       " 'just',\n",
       " 'than',\n",
       " 'centerofright',\n",
       " 'them',\n",
       " 'survey',\n",
       " 'once',\n",
       " 'most',\n",
       " 'here',\n",
       " 'watch',\n",
       " 'running',\n",
       " 'pmoindia',\n",
       " 'got',\n",
       " 'digital',\n",
       " 'shashitharoor',\n",
       " 'result',\n",
       " 'ahead',\n",
       " 'then',\n",
       " 'even',\n",
       " 'banks',\n",
       " 'parliament',\n",
       " 'note',\n",
       " 'indias',\n",
       " 'new',\n",
       " 'where',\n",
       " 'time',\n",
       " 'going',\n",
       " 'lost',\n",
       " 'get',\n",
       " 'videos',\n",
       " 'jan',\n",
       " 'opposition',\n",
       " 'take',\n",
       " 'life',\n",
       " 'dasshaktikanta',\n",
       " 'dhan',\n",
       " 'claims',\n",
       " 'wedding',\n",
       " 'historically',\n",
       " 'ujjwala',\n",
       " 'emandis',\n",
       " 'urea',\n",
       " 'coated',\n",
       " 'neem',\n",
       " 'during',\n",
       " 'mallyas',\n",
       " 'only',\n",
       " 'unprecedent',\n",
       " 'demo',\n",
       " 'yogi',\n",
       " 'withdrawal',\n",
       " 'adityanath',\n",
       " 'off',\n",
       " 'there',\n",
       " 'theres',\n",
       " 'ads',\n",
       " 'hanke',\n",
       " 'news',\n",
       " 'blackmoney',\n",
       " 'rbi',\n",
       " 'steve',\n",
       " 'restrictions',\n",
       " 'post',\n",
       " 'thought',\n",
       " 'loosened',\n",
       " 'think',\n",
       " 'economic',\n",
       " 'ban',\n",
       " 'indian',\n",
       " 'positive',\n",
       " 'system',\n",
       " 'days',\n",
       " 'wave',\n",
       " 'successful',\n",
       " 'prove',\n",
       " 'already',\n",
       " 'been',\n",
       " 'mamata',\n",
       " 'fund',\n",
       " 'decision',\n",
       " 'gujarat',\n",
       " 'arvindkejriwal',\n",
       " 'article',\n",
       " 'cant',\n",
       " 'seeing',\n",
       " 'video',\n",
       " 'did',\n",
       " 'join',\n",
       " 'daughter',\n",
       " 'ends',\n",
       " 'taking',\n",
       " 'kanimozhi',\n",
       " 'standing',\n",
       " 'believe',\n",
       " 'roflgandhi',\n",
       " 'world',\n",
       " 'askanshul',\n",
       " 'understand',\n",
       " 'patel',\n",
       " 'which',\n",
       " 'fake',\n",
       " 'far',\n",
       " 'ages',\n",
       " 'munaf',\n",
       " 'glvmi',\n",
       " 'timcast',\n",
       " 'day',\n",
       " 'daily',\n",
       " 'their',\n",
       " 'results',\n",
       " 'must',\n",
       " 'know',\n",
       " 'reinstate',\n",
       " 'political',\n",
       " 'services',\n",
       " 'benefits',\n",
       " 'thus',\n",
       " 'watched',\n",
       " 'peoples',\n",
       " 'flags',\n",
       " 'krishna',\n",
       " 'read',\n",
       " 'state',\n",
       " 'crores',\n",
       " 'oswal',\n",
       " 'motilal',\n",
       " 'one',\n",
       " 'through',\n",
       " 'atheist',\n",
       " 'some',\n",
       " 'kashmir',\n",
       " 'part',\n",
       " 'address',\n",
       " 'media',\n",
       " 'done',\n",
       " 'kerala',\n",
       " 'government',\n",
       " 'say',\n",
       " 'many',\n",
       " 'please',\n",
       " 'getting',\n",
       " 'less',\n",
       " 'corruption',\n",
       " 'pappu',\n",
       " 'come',\n",
       " 'feedback',\n",
       " 'report',\n",
       " 'strategy',\n",
       " 'seats',\n",
       " 'calling',\n",
       " 'proved',\n",
       " 'see',\n",
       " 'economics',\n",
       " 'among',\n",
       " 'line',\n",
       " 'payments',\n",
       " 'secy',\n",
       " 'dea',\n",
       " 'pib',\n",
       " 'ippatel',\n",
       " 'help',\n",
       " 'affected',\n",
       " 'kept',\n",
       " 'love',\n",
       " 'joydas',\n",
       " 'big',\n",
       " 'response',\n",
       " 'overall',\n",
       " 'retaining',\n",
       " 'byelection',\n",
       " 'supply',\n",
       " 'blames',\n",
       " 'education',\n",
       " 'makes',\n",
       " 'jamewils',\n",
       " 'worth',\n",
       " 'grow',\n",
       " 'need',\n",
       " 'abov',\n",
       " 'morning',\n",
       " 'briefing',\n",
       " 'lack',\n",
       " 'quiet',\n",
       " 'arunjaitley',\n",
       " 'staff',\n",
       " 'banned',\n",
       " 'heard',\n",
       " 'gandhi',\n",
       " 'loot',\n",
       " 'doesnt',\n",
       " 'spread',\n",
       " 'much',\n",
       " 'exactly',\n",
       " 'index',\n",
       " 'currency',\n",
       " 'stop',\n",
       " 'nationalists',\n",
       " 'theyll',\n",
       " 'walk',\n",
       " 'mas',\n",
       " 'crisis',\n",
       " 'finminindia',\n",
       " 'corruptionfreeindia',\n",
       " 'loan',\n",
       " 'headline',\n",
       " 'prohibition',\n",
       " 'yearend',\n",
       " 'attomeybharti',\n",
       " 'into',\n",
       " 'waived',\n",
       " 'closing',\n",
       " 'mens',\n",
       " 'real',\n",
       " 'tobaccogutkha',\n",
       " 'overwhelming',\n",
       " 'joydeep',\n",
       " 'englis',\n",
       " 'would',\n",
       " 'end',\n",
       " 'being',\n",
       " 'become',\n",
       " 'actions',\n",
       " 'give',\n",
       " 'indians',\n",
       " 'farmers',\n",
       " 'very',\n",
       " 'suspect',\n",
       " 'vote',\n",
       " 'politicsits',\n",
       " 'transparent',\n",
       " 'elec',\n",
       " 'reflected',\n",
       " 'those',\n",
       " 'piyushgoyaloffc',\n",
       " 'development',\n",
       " 'country',\n",
       " 'demonetisation',\n",
       " 'debate',\n",
       " 'ppl',\n",
       " 'doing',\n",
       " 'helped',\n",
       " 'telangana',\n",
       " 'these',\n",
       " 'another',\n",
       " 'nothing',\n",
       " 'long',\n",
       " 'drgpradhan',\n",
       " 'youre',\n",
       " 'public',\n",
       " 'minister',\n",
       " 'army',\n",
       " 'memeghnad',\n",
       " 'bad',\n",
       " 'explains',\n",
       " 'vijaymallya',\n",
       " 'thanks',\n",
       " 'old',\n",
       " 'protest',\n",
       " 'pet',\n",
       " 'mps',\n",
       " 'bailed',\n",
       " 'windfall',\n",
       " 'said',\n",
       " 'security',\n",
       " 'same',\n",
       " 'argument',\n",
       " 'didnt',\n",
       " 'online',\n",
       " 'citizens',\n",
       " 'having',\n",
       " 'everyone',\n",
       " 'acc',\n",
       " 'announcements',\n",
       " 'http',\n",
       " 'tweet',\n",
       " 'border',\n",
       " 'banking',\n",
       " 'yet',\n",
       " 'asian',\n",
       " 'atm',\n",
       " 'fight',\n",
       " 'profit',\n",
       " 'dna',\n",
       " 'drama',\n",
       " 'positiv',\n",
       " 'poll',\n",
       " 'other',\n",
       " 'mib',\n",
       " 'could',\n",
       " 'robberies',\n",
       " 'indiafightscorruption',\n",
       " 'iip',\n",
       " 'routine',\n",
       " 'contracts',\n",
       " 'inconvenience',\n",
       " 'truth',\n",
       " 'because',\n",
       " 'prime',\n",
       " 'check',\n",
       " 'whooping',\n",
       " 'reason',\n",
       " 'along',\n",
       " 'joined',\n",
       " 'thore',\n",
       " 'prob',\n",
       " 'parlmnt',\n",
       " 'force',\n",
       " 'minimathur',\n",
       " 'coast',\n",
       " 'fishermen',\n",
       " 'made',\n",
       " 'also',\n",
       " 'down',\n",
       " 'poo',\n",
       " 'bibi',\n",
       " 'miteshpatel',\n",
       " 'useless',\n",
       " 'lot',\n",
       " 'seizes',\n",
       " 'years',\n",
       " 'bengals',\n",
       " 'malda',\n",
       " 'despite',\n",
       " 'dog',\n",
       " 'solidarity',\n",
       " 'opposing',\n",
       " 'well',\n",
       " 'coming',\n",
       " 'make',\n",
       " 'asked',\n",
       " 'wants',\n",
       " 'last',\n",
       " 'hours',\n",
       " 'liked',\n",
       " 'feel',\n",
       " 'aaanupriyaaa',\n",
       " 'issue',\n",
       " 'scam',\n",
       " 'growth',\n",
       " 'seems',\n",
       " 'amazing',\n",
       " 'demonstrating',\n",
       " 'fall',\n",
       " 'majority',\n",
       " 'etc',\n",
       " 'withinc',\n",
       " 'demanding',\n",
       " 'implementation',\n",
       " 'idea',\n",
       " 'corrupt',\n",
       " 'friends',\n",
       " 'while',\n",
       " 'usual',\n",
       " 'folks',\n",
       " 'right',\n",
       " 'parliame',\n",
       " 'hemant',\n",
       " 'away',\n",
       " 'bengal',\n",
       " 'needed',\n",
       " 'global',\n",
       " 'htt',\n",
       " 'used',\n",
       " 'google',\n",
       " 'cashless',\n",
       " 'govts',\n",
       " 'latest',\n",
       " 'sir',\n",
       " 'voted',\n",
       " 'sonunigam',\n",
       " 'free',\n",
       " 'biggest',\n",
       " 'jairajp',\n",
       " 'demonetizat',\n",
       " 'major',\n",
       " 'opinions',\n",
       " 'comes',\n",
       " 'views',\n",
       " 'income',\n",
       " 'put',\n",
       " 'technology',\n",
       " 'price',\n",
       " 'pan',\n",
       " 'drive',\n",
       " 'means',\n",
       " 'told',\n",
       " 'harshkkapoor',\n",
       " 'stone',\n",
       " 'weeks',\n",
       " 'parties',\n",
       " 'tax',\n",
       " 'ficn',\n",
       " 'service',\n",
       " 'keep',\n",
       " 'thing',\n",
       " 'production',\n",
       " 'terrorism',\n",
       " 'commitment',\n",
       " 'taken',\n",
       " 'charges',\n",
       " 'censorship',\n",
       " 'indeed',\n",
       " 'lead',\n",
       " 'double',\n",
       " 'ahmedabad',\n",
       " 'queues',\n",
       " 'duty',\n",
       " 'hilarious',\n",
       " 'solution',\n",
       " 'shirishkunder',\n",
       " 'making',\n",
       " 'gone',\n",
       " 'writes',\n",
       " 'story',\n",
       " 'wil',\n",
       " 'speech',\n",
       " 'frankly',\n",
       " 'none',\n",
       " 'vijay',\n",
       " 'delhi',\n",
       " 'industrial',\n",
       " 'action',\n",
       " 'anandkamal',\n",
       " 'gets',\n",
       " 'issues',\n",
       " 'too',\n",
       " 'consumer',\n",
       " 'thank',\n",
       " 'bcoz',\n",
       " 'aap',\n",
       " 'lol',\n",
       " 'prices',\n",
       " 'nomoneyyaar',\n",
       " 'common',\n",
       " 'maoists',\n",
       " 'deep',\n",
       " 'may',\n",
       " 'dipendradipzo',\n",
       " 'cpi',\n",
       " 'whammy',\n",
       " 'rakshaksetc',\n",
       " 'mallya',\n",
       " 'problem',\n",
       " 'drivesgau',\n",
       " 'demonetizationantiromeo',\n",
       " 'crunch',\n",
       " 'mean',\n",
       " 'shop',\n",
       " 'stunts',\n",
       " 'swachata',\n",
       " 'diwasyoga',\n",
       " 'finally',\n",
       " 'loss',\n",
       " 'owner',\n",
       " 'mkatju',\n",
       " 'exchange',\n",
       " 'tarekfatah',\n",
       " 'hitler',\n",
       " 'way',\n",
       " 'paan',\n",
       " 'true',\n",
       " 'anything',\n",
       " 'richer',\n",
       " 'per',\n",
       " 'hates',\n",
       " 'severe',\n",
       " 'historic',\n",
       " 'honest',\n",
       " 'liquidity',\n",
       " 'worldbank',\n",
       " 'decide',\n",
       " 'initiative',\n",
       " 'immense',\n",
       " 'months',\n",
       " 'snapchat',\n",
       " 'live',\n",
       " 'watching',\n",
       " 'minor',\n",
       " 'proves',\n",
       " 'mahcongress',\n",
       " 'behind',\n",
       " 'innovative',\n",
       " 'yes',\n",
       " 'rural',\n",
       " 'exports',\n",
       " 'convenience',\n",
       " 'might',\n",
       " 'change',\n",
       " 'taught',\n",
       " 'increased',\n",
       " 'police',\n",
       " 'happiness',\n",
       " 'totally',\n",
       " 'market',\n",
       " 'dipankar',\n",
       " 'stuff',\n",
       " 'failure',\n",
       " 'march',\n",
       " 'humorministry',\n",
       " 'congress',\n",
       " 'supporting',\n",
       " 'shefvaidya',\n",
       " 'minimalism',\n",
       " 'given',\n",
       " 'praising',\n",
       " 'insights',\n",
       " 'boycottsnapchat',\n",
       " 'madskak',\n",
       " 'hit',\n",
       " 'arvindsubraman',\n",
       " 'continues',\n",
       " 'officeofrg',\n",
       " 'milkman',\n",
       " 'data',\n",
       " 'stupidosaur',\n",
       " 'swift',\n",
       " 'policy',\n",
       " 'deployment',\n",
       " 'dire',\n",
       " 'bulletin',\n",
       " 'around',\n",
       " 'drshobha',\n",
       " 'two',\n",
       " 'reconfigured',\n",
       " 'ratantata',\n",
       " 'open',\n",
       " 'prasadaol',\n",
       " 'courage',\n",
       " 'announcement',\n",
       " 'rashminp',\n",
       " 'cpiml',\n",
       " 'lines',\n",
       " 'step',\n",
       " 'queue',\n",
       " 'indifferent',\n",
       " 'barbarindian',\n",
       " 'trust',\n",
       " 'mounting',\n",
       " 'misery',\n",
       " 'indiatoday',\n",
       " 'gives',\n",
       " 'shut',\n",
       " 'white',\n",
       " 'success',\n",
       " 'started',\n",
       " 'use',\n",
       " 'using',\n",
       " 'calls',\n",
       " 'high',\n",
       " 'tell',\n",
       " 'reduced',\n",
       " 'minit',\n",
       " 'planned',\n",
       " 'leader',\n",
       " 'themselves',\n",
       " 'hards',\n",
       " 'reports',\n",
       " 'secretarymeity',\n",
       " 'realhistorypic',\n",
       " 'annielowrey',\n",
       " 'duplicate',\n",
       " 'effects',\n",
       " 'both',\n",
       " 'reminds',\n",
       " 'momentous',\n",
       " 'buried',\n",
       " 'batch',\n",
       " 'deaths',\n",
       " 'convenientl',\n",
       " 'reaction',\n",
       " 'war',\n",
       " 'responsible',\n",
       " 'team',\n",
       " 'america',\n",
       " 'start',\n",
       " 'ncbn',\n",
       " 'ysrcparty',\n",
       " 'creating',\n",
       " 'small',\n",
       " 'guess',\n",
       " 'social',\n",
       " 'thoughts',\n",
       " 'turning',\n",
       " 'things',\n",
       " 'thedarjchron',\n",
       " 'business',\n",
       " 'recovered',\n",
       " 'run',\n",
       " 'protesting',\n",
       " 'withdraw',\n",
       " 'analysis',\n",
       " 'hour',\n",
       " 'medical',\n",
       " 'hope',\n",
       " 'mamataofficial',\n",
       " 'large',\n",
       " 'pelting',\n",
       " 'evm',\n",
       " 'beginning',\n",
       " 'wiped',\n",
       " 'scheme',\n",
       " 'bring',\n",
       " 'paid',\n",
       " 'looking',\n",
       " 'blunder',\n",
       " 'really',\n",
       " 'set',\n",
       " 'mobile',\n",
       " 'under',\n",
       " 'attempt',\n",
       " 'prof',\n",
       " 'myvotetoday',\n",
       " 'evident',\n",
       " 'bandwagon',\n",
       " 'censoring',\n",
       " 'mudra',\n",
       " 'exercise',\n",
       " 'vindicated',\n",
       " 'giving',\n",
       " 'caused',\n",
       " 'next',\n",
       " 'suffered',\n",
       " 'shiralkar',\n",
       " 'never',\n",
       " 'cgdev',\n",
       " 'massive',\n",
       " 'ques',\n",
       " 'national',\n",
       " 'happy',\n",
       " 'hate',\n",
       " 'history',\n",
       " 'chief',\n",
       " 'banerjee',\n",
       " 'chamchas',\n",
       " 'pakistan',\n",
       " 'let',\n",
       " 'jpal',\n",
       " 'able',\n",
       " 'body',\n",
       " 'parliamenta',\n",
       " 'deposits',\n",
       " 'street',\n",
       " 'anyone',\n",
       " 'crore',\n",
       " 'disaster',\n",
       " 'vaidyanathan',\n",
       " 'delisting',\n",
       " 'sleeps',\n",
       " 'sales',\n",
       " 'lifeinsurance',\n",
       " 'financial',\n",
       " 'better',\n",
       " 'hoarders',\n",
       " 'every',\n",
       " 'stand',\n",
       " 'propaganda',\n",
       " 'though',\n",
       " 'rules',\n",
       " 'crossed',\n",
       " 'fingers',\n",
       " 'coop',\n",
       " 'towards',\n",
       " 'wasnt',\n",
       " 'wise',\n",
       " 'want',\n",
       " 'steps',\n",
       " 'lionelmedia',\n",
       " 'rana',\n",
       " 'jio',\n",
       " 'tomorrow',\n",
       " 'interesting',\n",
       " 'great',\n",
       " 'rise',\n",
       " 'bajajallianzlic',\n",
       " 'productivity',\n",
       " 'died',\n",
       " 'happened',\n",
       " 'secure',\n",
       " 'current',\n",
       " 'milanv',\n",
       " 'pmmodi',\n",
       " 'jumla',\n",
       " 'estate',\n",
       " 'mantar',\n",
       " 'amazingatheist',\n",
       " 'trxns',\n",
       " 'suffering',\n",
       " 'jantar',\n",
       " 'percent',\n",
       " 'markets',\n",
       " 'opposes',\n",
       " 'indiaexplained',\n",
       " 'gold',\n",
       " 'outside',\n",
       " 'west',\n",
       " 'narendramodis',\n",
       " 'notice',\n",
       " 'msisodia',\n",
       " 'demand',\n",
       " 'mode',\n",
       " 'does',\n",
       " 'postdemonetization',\n",
       " 'ndtv',\n",
       " 'waiting',\n",
       " 'creativity',\n",
       " 'prior',\n",
       " 'janamejayan',\n",
       " 'dry',\n",
       " 'sold',\n",
       " 'cag',\n",
       " 'found',\n",
       " 'till',\n",
       " 'audit',\n",
       " 'killed',\n",
       " 'boycott',\n",
       " 'download',\n",
       " 'hrs',\n",
       " 'toss',\n",
       " 'royal',\n",
       " 'goalpost',\n",
       " 'hear',\n",
       " 'transactions',\n",
       " 'station',\n",
       " 'markdice',\n",
       " 'plz',\n",
       " 'suspects',\n",
       " 'noises',\n",
       " 'left',\n",
       " 'aadhaar',\n",
       " 'abt',\n",
       " 'knowledge',\n",
       " 'points',\n",
       " 'times',\n",
       " 'working',\n",
       " 'scale',\n",
       " 'fastest',\n",
       " 'continue',\n",
       " 'sanjayuv',\n",
       " 'uturned',\n",
       " 'detaiend',\n",
       " 'cpim',\n",
       " 'pawar',\n",
       " 'appearing',\n",
       " 'tragic',\n",
       " 'earlier',\n",
       " 'ways',\n",
       " 'paresh',\n",
       " 'bsindia',\n",
       " 'sim',\n",
       " 'catalyzing',\n",
       " 'supported',\n",
       " 'pay',\n",
       " 'offensive',\n",
       " 'riffin',\n",
       " 'helping',\n",
       " 'youtubeisdead',\n",
       " 'enough',\n",
       " 'slip',\n",
       " 'unaffected',\n",
       " 'answer',\n",
       " 'value',\n",
       " 'coverage',\n",
       " 'incometaxindia',\n",
       " 'everything',\n",
       " 'nmapp',\n",
       " 'view',\n",
       " 'negative',\n",
       " 'without',\n",
       " 'phase',\n",
       " 'tells',\n",
       " 'panic',\n",
       " 'isnt',\n",
       " 'funds',\n",
       " 'seriously',\n",
       " 'road',\n",
       " 'term',\n",
       " 'gdp',\n",
       " 'sources',\n",
       " 'license',\n",
       " 'created',\n",
       " 'required',\n",
       " 'push',\n",
       " 'purchases',\n",
       " 'airnewsalerts',\n",
       " 'jackerhack',\n",
       " 'mla',\n",
       " 'sbi',\n",
       " 'hurt',\n",
       " 'polls',\n",
       " 'situation',\n",
       " 'explained',\n",
       " 'sharesclearly',\n",
       " 'heritage',\n",
       " 'thug',\n",
       " 'show',\n",
       " 'evms',\n",
       " 'cut',\n",
       " 'complete',\n",
       " 'minhazmerchant',\n",
       " 'meradeshbadalraha',\n",
       " 'answered',\n",
       " 'punitspeaks',\n",
       " 'struggle',\n",
       " 'card',\n",
       " 'supposed',\n",
       " 'modiji',\n",
       " 'shopping',\n",
       " 'indores',\n",
       " 'collector',\n",
       " 'research',\n",
       " 'mandates',\n",
       " 'internetfreedom',\n",
       " 'rupasubramanya',\n",
       " 'digvijaya',\n",
       " 'allowed',\n",
       " 'delhities',\n",
       " 'cursing',\n",
       " 'transaction',\n",
       " 'kejriwal',\n",
       " 'urjitpatelrbi',\n",
       " 'yesiamsaffron',\n",
       " 'february',\n",
       " 'adsense',\n",
       " 'clean',\n",
       " 'soon',\n",
       " 'recalibrated',\n",
       " 'exclusive',\n",
       " 'sector',\n",
       " 'missed',\n",
       " 'took',\n",
       " 'bold',\n",
       " 'netascashin',\n",
       " 'hpcl',\n",
       " 'disruption',\n",
       " 'cmomaharashtra',\n",
       " 'terror',\n",
       " 'tmc',\n",
       " 'death',\n",
       " 'richest',\n",
       " 'bsp',\n",
       " 'link',\n",
       " 'hard',\n",
       " 'takes',\n",
       " 'following',\n",
       " 'participate',\n",
       " 'formation',\n",
       " 'payment',\n",
       " 'subverting',\n",
       " 'lingers',\n",
       " 'singh',\n",
       " 'rkhuria',\n",
       " 'power',\n",
       " 'planning',\n",
       " 'serve',\n",
       " 'kilafateh',\n",
       " 'hand',\n",
       " 'sad',\n",
       " 'anchor',\n",
       " 'riots',\n",
       " 'struggling',\n",
       " 'civil',\n",
       " 'opinion',\n",
       " 'season',\n",
       " 'ideas',\n",
       " 'trustbjp',\n",
       " 'wake',\n",
       " 'allowing',\n",
       " 'unable',\n",
       " 'demon',\n",
       " 'court',\n",
       " 'justify',\n",
       " 'ind',\n",
       " 'least',\n",
       " 'trending',\n",
       " 'dept',\n",
       " 'nov',\n",
       " 'month',\n",
       " 'bigger',\n",
       " 'opportunities',\n",
       " 'listen',\n",
       " 'always',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(model.wv.key_to_index)\n",
    "def sentence_vector(sentence, model):\n",
    "    nwords = 0\n",
    "    featureV = np.zeros(100, dtype=\"float32\")\n",
    "    for word in sentence:\n",
    "        if word not in vocab:\n",
    "            continue\n",
    "        featureV = np.add(featureV, model.wv[word])\n",
    "        nwords = nwords + 1\n",
    "    if nwords > 0: \n",
    "        featureV = np.divide(featureV, nwords)\n",
    "    return featureV\n",
    "\n",
    "tweet_vector = df['tokenized_tweet'].apply(lambda x: sentence_vector(x, model))  \n",
    "\n",
    "tweet_vector = tweet_vector.apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweet vector should vary from 0 to 1 (normalize the vector)\n",
    "for x in range(len(tweet_vector)):\n",
    "    x_min = tweet_vector.iloc[x].min()\n",
    "    x_max = tweet_vector.iloc[x].max()\n",
    "    X  = tweet_vector.iloc[x]\n",
    "    i = 0\n",
    "    if (x_max - x_min) == 0:\n",
    "        for y in X:\n",
    "            tweet_vector.iloc[x][i] = (1/len(tweet_vector.iloc[x]))\n",
    "            i = i + 1\n",
    "    else:\n",
    "        for y in X:\n",
    "            tweet_vector.iloc[x][i] = ((y - x_min)/(x_max - x_min))\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             0         1         2         3         4         5         6    \n",
      "0     -0.119772  0.012541  0.022908 -0.012207  0.093855 -0.107808  0.065990  \\\n",
      "1     -0.151624  0.067410  0.001232  0.033011  0.012311 -0.053566  0.063053   \n",
      "2     -0.109865  0.079275 -0.036803  0.069708  0.015164 -0.128124  0.172709   \n",
      "3     -0.091764  0.031438 -0.030831  0.067390  0.020482 -0.175661  0.157000   \n",
      "4     -0.070429  0.102370  0.042013  0.006973  0.018645  0.000000  0.194049   \n",
      "...         ...       ...       ...       ...       ...       ...       ...   \n",
      "14935 -0.071265  0.062739 -0.027764  0.063051  0.047983 -0.116335  0.138535   \n",
      "14936 -0.108562  0.069429 -0.043389  0.065047  0.022246 -0.092271  0.109841   \n",
      "14937 -0.093982  0.031785 -0.115378  0.073162 -0.010726 -0.047210  0.154450   \n",
      "14938 -0.069449  0.029856 -0.053555  0.064788  0.055495 -0.078229  0.143374   \n",
      "14939 -0.065060  0.028290 -0.058571  0.071047  0.054906 -0.065682  0.145076   \n",
      "\n",
      "             7         8         9   ...        90        91        92   \n",
      "0      0.193632  0.124113  0.073122  ...  0.022574  0.128465  0.047848  \\\n",
      "1      0.145116  0.120084  0.106455  ... -0.085436  0.033400  0.034057   \n",
      "2      0.149966  0.103798  0.108282  ...  0.009599  0.050000 -0.007982   \n",
      "3      0.155331  0.066747  0.067203  ...  0.046448  0.038812 -0.020445   \n",
      "4      0.108037  0.021510  0.047469  ...  0.004784  0.039842  0.063686   \n",
      "...         ...       ...       ...  ...       ...       ...       ...   \n",
      "14935  0.078707  0.035290  0.075781  ...  0.024476  0.030878 -0.063327   \n",
      "14936  0.141071  0.107169  0.090184  ...  0.008258  0.044461  0.029142   \n",
      "14937  0.049487  0.102480  0.076090  ...  0.026661  0.037369 -0.051818   \n",
      "14938  0.137145  0.123738  0.092735  ...  0.032395  0.038365 -0.025350   \n",
      "14939  0.124254  0.127909  0.091932  ...  0.038044  0.036967 -0.033880   \n",
      "\n",
      "             93        94        95        96        97        98        99  \n",
      "0     -0.132761  1.000000  0.103569  0.035771  0.000000 -0.145036  0.065566  \n",
      "1     -0.122641  1.000000  0.078306 -0.007952 -0.163343 -0.068051  0.080755  \n",
      "2     -0.064162  0.191141  0.221205 -0.044450 -0.139647 -0.128860  0.063375  \n",
      "3     -0.052864  0.147307  0.228458 -0.040328 -0.134557 -0.146528  0.043255  \n",
      "4     -0.000034  0.165006  0.174933  0.021646 -0.140149 -0.032286  0.092191  \n",
      "...         ...       ...       ...       ...       ...       ...       ...  \n",
      "14935 -0.006459  0.071347  0.157959 -0.041671 -0.092269 -0.112490  0.016546  \n",
      "14936 -0.105415  0.157596  0.155227 -0.028157 -0.108655 -0.143632  0.053702  \n",
      "14937 -0.079735  0.068371  0.179009 -0.081439 -0.060286 -0.143351  0.050386  \n",
      "14938 -0.103527  0.146800  0.145423 -0.039902 -0.070741 -0.142590  0.067799  \n",
      "14939 -0.100707  0.135245  0.144843 -0.043026 -0.057590 -0.147224  0.065750  \n",
      "\n",
      "[14940 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "print(tweet_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Add sentiment to the tweet vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the 'sentiment' vector\n",
    "#Sentiment varies from -1 to +1\n",
    "\n",
    "def sentiment(x):\n",
    "    if x < 0.04:\n",
    "        return 0\n",
    "    elif x > 0.04:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0.5\n",
    "\n",
    "tweet_vector[100] = df['sentiment'].apply(lambda x: sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.119772</td>\n",
       "      <td>0.012541</td>\n",
       "      <td>0.022908</td>\n",
       "      <td>-0.012207</td>\n",
       "      <td>0.093855</td>\n",
       "      <td>-0.107808</td>\n",
       "      <td>0.065990</td>\n",
       "      <td>0.193632</td>\n",
       "      <td>0.124113</td>\n",
       "      <td>0.073122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128465</td>\n",
       "      <td>0.047848</td>\n",
       "      <td>-0.132761</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.103569</td>\n",
       "      <td>0.035771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.145036</td>\n",
       "      <td>0.065566</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.151624</td>\n",
       "      <td>0.067410</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.033011</td>\n",
       "      <td>0.012311</td>\n",
       "      <td>-0.053566</td>\n",
       "      <td>0.063053</td>\n",
       "      <td>0.145116</td>\n",
       "      <td>0.120084</td>\n",
       "      <td>0.106455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>0.034057</td>\n",
       "      <td>-0.122641</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.078306</td>\n",
       "      <td>-0.007952</td>\n",
       "      <td>-0.163343</td>\n",
       "      <td>-0.068051</td>\n",
       "      <td>0.080755</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.109865</td>\n",
       "      <td>0.079275</td>\n",
       "      <td>-0.036803</td>\n",
       "      <td>0.069708</td>\n",
       "      <td>0.015164</td>\n",
       "      <td>-0.128124</td>\n",
       "      <td>0.172709</td>\n",
       "      <td>0.149966</td>\n",
       "      <td>0.103798</td>\n",
       "      <td>0.108282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.007982</td>\n",
       "      <td>-0.064162</td>\n",
       "      <td>0.191141</td>\n",
       "      <td>0.221205</td>\n",
       "      <td>-0.044450</td>\n",
       "      <td>-0.139647</td>\n",
       "      <td>-0.128860</td>\n",
       "      <td>0.063375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.091764</td>\n",
       "      <td>0.031438</td>\n",
       "      <td>-0.030831</td>\n",
       "      <td>0.067390</td>\n",
       "      <td>0.020482</td>\n",
       "      <td>-0.175661</td>\n",
       "      <td>0.157000</td>\n",
       "      <td>0.155331</td>\n",
       "      <td>0.066747</td>\n",
       "      <td>0.067203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038812</td>\n",
       "      <td>-0.020445</td>\n",
       "      <td>-0.052864</td>\n",
       "      <td>0.147307</td>\n",
       "      <td>0.228458</td>\n",
       "      <td>-0.040328</td>\n",
       "      <td>-0.134557</td>\n",
       "      <td>-0.146528</td>\n",
       "      <td>0.043255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.070429</td>\n",
       "      <td>0.102370</td>\n",
       "      <td>0.042013</td>\n",
       "      <td>0.006973</td>\n",
       "      <td>0.018645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.194049</td>\n",
       "      <td>0.108037</td>\n",
       "      <td>0.021510</td>\n",
       "      <td>0.047469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039842</td>\n",
       "      <td>0.063686</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>0.165006</td>\n",
       "      <td>0.174933</td>\n",
       "      <td>0.021646</td>\n",
       "      <td>-0.140149</td>\n",
       "      <td>-0.032286</td>\n",
       "      <td>0.092191</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14935</th>\n",
       "      <td>-0.071265</td>\n",
       "      <td>0.062739</td>\n",
       "      <td>-0.027764</td>\n",
       "      <td>0.063051</td>\n",
       "      <td>0.047983</td>\n",
       "      <td>-0.116335</td>\n",
       "      <td>0.138535</td>\n",
       "      <td>0.078707</td>\n",
       "      <td>0.035290</td>\n",
       "      <td>0.075781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030878</td>\n",
       "      <td>-0.063327</td>\n",
       "      <td>-0.006459</td>\n",
       "      <td>0.071347</td>\n",
       "      <td>0.157959</td>\n",
       "      <td>-0.041671</td>\n",
       "      <td>-0.092269</td>\n",
       "      <td>-0.112490</td>\n",
       "      <td>0.016546</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14936</th>\n",
       "      <td>-0.108562</td>\n",
       "      <td>0.069429</td>\n",
       "      <td>-0.043389</td>\n",
       "      <td>0.065047</td>\n",
       "      <td>0.022246</td>\n",
       "      <td>-0.092271</td>\n",
       "      <td>0.109841</td>\n",
       "      <td>0.141071</td>\n",
       "      <td>0.107169</td>\n",
       "      <td>0.090184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044461</td>\n",
       "      <td>0.029142</td>\n",
       "      <td>-0.105415</td>\n",
       "      <td>0.157596</td>\n",
       "      <td>0.155227</td>\n",
       "      <td>-0.028157</td>\n",
       "      <td>-0.108655</td>\n",
       "      <td>-0.143632</td>\n",
       "      <td>0.053702</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14937</th>\n",
       "      <td>-0.093982</td>\n",
       "      <td>0.031785</td>\n",
       "      <td>-0.115378</td>\n",
       "      <td>0.073162</td>\n",
       "      <td>-0.010726</td>\n",
       "      <td>-0.047210</td>\n",
       "      <td>0.154450</td>\n",
       "      <td>0.049487</td>\n",
       "      <td>0.102480</td>\n",
       "      <td>0.076090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037369</td>\n",
       "      <td>-0.051818</td>\n",
       "      <td>-0.079735</td>\n",
       "      <td>0.068371</td>\n",
       "      <td>0.179009</td>\n",
       "      <td>-0.081439</td>\n",
       "      <td>-0.060286</td>\n",
       "      <td>-0.143351</td>\n",
       "      <td>0.050386</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14938</th>\n",
       "      <td>-0.069449</td>\n",
       "      <td>0.029856</td>\n",
       "      <td>-0.053555</td>\n",
       "      <td>0.064788</td>\n",
       "      <td>0.055495</td>\n",
       "      <td>-0.078229</td>\n",
       "      <td>0.143374</td>\n",
       "      <td>0.137145</td>\n",
       "      <td>0.123738</td>\n",
       "      <td>0.092735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038365</td>\n",
       "      <td>-0.025350</td>\n",
       "      <td>-0.103527</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>0.145423</td>\n",
       "      <td>-0.039902</td>\n",
       "      <td>-0.070741</td>\n",
       "      <td>-0.142590</td>\n",
       "      <td>0.067799</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14939</th>\n",
       "      <td>-0.065060</td>\n",
       "      <td>0.028290</td>\n",
       "      <td>-0.058571</td>\n",
       "      <td>0.071047</td>\n",
       "      <td>0.054906</td>\n",
       "      <td>-0.065682</td>\n",
       "      <td>0.145076</td>\n",
       "      <td>0.124254</td>\n",
       "      <td>0.127909</td>\n",
       "      <td>0.091932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036967</td>\n",
       "      <td>-0.033880</td>\n",
       "      <td>-0.100707</td>\n",
       "      <td>0.135245</td>\n",
       "      <td>0.144843</td>\n",
       "      <td>-0.043026</td>\n",
       "      <td>-0.057590</td>\n",
       "      <td>-0.147224</td>\n",
       "      <td>0.065750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14940 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6     \n",
       "0     -0.119772  0.012541  0.022908 -0.012207  0.093855 -0.107808  0.065990  \\\n",
       "1     -0.151624  0.067410  0.001232  0.033011  0.012311 -0.053566  0.063053   \n",
       "2     -0.109865  0.079275 -0.036803  0.069708  0.015164 -0.128124  0.172709   \n",
       "3     -0.091764  0.031438 -0.030831  0.067390  0.020482 -0.175661  0.157000   \n",
       "4     -0.070429  0.102370  0.042013  0.006973  0.018645  0.000000  0.194049   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "14935 -0.071265  0.062739 -0.027764  0.063051  0.047983 -0.116335  0.138535   \n",
       "14936 -0.108562  0.069429 -0.043389  0.065047  0.022246 -0.092271  0.109841   \n",
       "14937 -0.093982  0.031785 -0.115378  0.073162 -0.010726 -0.047210  0.154450   \n",
       "14938 -0.069449  0.029856 -0.053555  0.064788  0.055495 -0.078229  0.143374   \n",
       "14939 -0.065060  0.028290 -0.058571  0.071047  0.054906 -0.065682  0.145076   \n",
       "\n",
       "            7         8         9    ...       91        92        93    \n",
       "0      0.193632  0.124113  0.073122  ...  0.128465  0.047848 -0.132761  \\\n",
       "1      0.145116  0.120084  0.106455  ...  0.033400  0.034057 -0.122641   \n",
       "2      0.149966  0.103798  0.108282  ...  0.050000 -0.007982 -0.064162   \n",
       "3      0.155331  0.066747  0.067203  ...  0.038812 -0.020445 -0.052864   \n",
       "4      0.108037  0.021510  0.047469  ...  0.039842  0.063686 -0.000034   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "14935  0.078707  0.035290  0.075781  ...  0.030878 -0.063327 -0.006459   \n",
       "14936  0.141071  0.107169  0.090184  ...  0.044461  0.029142 -0.105415   \n",
       "14937  0.049487  0.102480  0.076090  ...  0.037369 -0.051818 -0.079735   \n",
       "14938  0.137145  0.123738  0.092735  ...  0.038365 -0.025350 -0.103527   \n",
       "14939  0.124254  0.127909  0.091932  ...  0.036967 -0.033880 -0.100707   \n",
       "\n",
       "            94        95        96        97        98        99   100  \n",
       "0      1.000000  0.103569  0.035771  0.000000 -0.145036  0.065566    1  \n",
       "1      1.000000  0.078306 -0.007952 -0.163343 -0.068051  0.080755    0  \n",
       "2      0.191141  0.221205 -0.044450 -0.139647 -0.128860  0.063375    0  \n",
       "3      0.147307  0.228458 -0.040328 -0.134557 -0.146528  0.043255    0  \n",
       "4      0.165006  0.174933  0.021646 -0.140149 -0.032286  0.092191    0  \n",
       "...         ...       ...       ...       ...       ...       ...  ...  \n",
       "14935  0.071347  0.157959 -0.041671 -0.092269 -0.112490  0.016546    0  \n",
       "14936  0.157596  0.155227 -0.028157 -0.108655 -0.143632  0.053702    0  \n",
       "14937  0.068371  0.179009 -0.081439 -0.060286 -0.143351  0.050386    1  \n",
       "14938  0.146800  0.145423 -0.039902 -0.070741 -0.142590  0.067799    0  \n",
       "14939  0.135245  0.144843 -0.043026 -0.057590 -0.147224  0.065750    0  \n",
       "\n",
       "[14940 rows x 101 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_vector  #sentiment 0 to +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating the 'sentiment' column in df also\n",
    "df['sentiment'] = tweet_vector[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Cluster the narratives [= opinions + expressions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 4 The average silhouette_score is : 0.280086391610312\n",
      "For n_clusters = 5 The average silhouette_score is : 0.3193228444215377\n",
      "For n_clusters = 6 The average silhouette_score is : 0.3438172617058945\n",
      "For n_clusters = 7 The average silhouette_score is : 0.3755095998110072\n",
      "For n_clusters = 8 The average silhouette_score is : 0.3976750583156838\n",
      "For n_clusters = 9 The average silhouette_score is : 0.41230399283149943\n",
      "For n_clusters = 10 The average silhouette_score is : 0.42697608236465606\n",
      "For n_clusters = 11 The average silhouette_score is : 0.44016286295969326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "range_n_clusters = [4, 5, 6, 7, 8, 9, 10, 11]\n",
    "X = tweet_vector\n",
    "n_best_clusters = 0\n",
    "silhouette_best = 0\n",
    "for n_clusters in range_n_clusters:\n",
    "    \n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10, n_init=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "                                      #, sample_size = 5000)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "    \n",
    "    if silhouette_avg > silhouette_best:\n",
    "        silhouette_best = silhouette_avg\n",
    "        n_best_clusters = n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_best_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = KMeans(n_clusters= n_best_clusters , random_state=10, n_init=10)\n",
    "cluster_labels = clusterer.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(cluster_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array of tweets, the corresponding cluster number, sentiment\n",
    "finaldf = pd.DataFrame({'cl_num': cluster_labels,'fully_cleaned_tweet': df['fully_cleaned_tweet'], 'cleaned_tweet': df['cleaned_tweet'], 'tweet': df['tweet'],'sentiment': df['sentiment']})\n",
    "finaldf = finaldf.sort_values(by=['cl_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cl_num'] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>fully_cleaned_tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>cl_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @rssurjewala: Critical question: Was PayTM ...</td>\n",
       "      <td>rssurjewala  critical question  was paytm info...</td>\n",
       "      <td>rssurjewala critical question was paytm inform...</td>\n",
       "      <td>1</td>\n",
       "      <td>[rssurjewala, critical, question, was, paytm, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Hemant_80: Did you vote on #Demonetization...</td>\n",
       "      <td>hemant 80  did you vote   demonetization  modi...</td>\n",
       "      <td>hemant 80 did you vote demonetization modi sur...</td>\n",
       "      <td>0</td>\n",
       "      <td>[hemant, did, you, vote, demonetization, modi,...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @roshankar: Former FinSec, RBI Dy Governor,...</td>\n",
       "      <td>roshankar  former finsec rbi  governor cbdt ch...</td>\n",
       "      <td>roshankar former finsec rbi governor cbdt chai...</td>\n",
       "      <td>0</td>\n",
       "      <td>[roshankar, former, finsec, rbi, governor, cbd...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @ANI_news: Gurugram (Haryana): Post office ...</td>\n",
       "      <td>ani news  gurugram haryana  post office employ...</td>\n",
       "      <td>ani news gurugram haryana post office employee...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ani, news, gurugram, haryana, post, office, e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @satishacharya: Reddy Wedding! @mail_today ...</td>\n",
       "      <td>satishacharya  reddy wedding   mail today cart...</td>\n",
       "      <td>satishacharya reddy wedding mail today cartoon...</td>\n",
       "      <td>0</td>\n",
       "      <td>[satishacharya, reddy, wedding, mail, today, c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14935</th>\n",
       "      <td>RT @saxenavishakha: Ghost of demonetization re...</td>\n",
       "      <td>saxenavishakha  ghost  demonetization returns ...</td>\n",
       "      <td>saxenavishakha ghost demonetization returns wi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[saxenavishakha, ghost, demonetization, return...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14936</th>\n",
       "      <td>N d modi fans-d true nationalists of the count...</td>\n",
       "      <td>modi fansd true nationalists  the country sti...</td>\n",
       "      <td>modi fansd true nationalists the country stil ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[modi, fansd, true, nationalists, the, country...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14937</th>\n",
       "      <td>RT @bharat_builder: Lol. Demonetization has fi...</td>\n",
       "      <td>bharat builder  lol demonetization has fixed  ...</td>\n",
       "      <td>bharat builder lol demonetization has fixed lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[bharat, builder, lol, demonetization, has, fi...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14938</th>\n",
       "      <td>RT @Stupidosaur: @Vidyut B team of BJP. CIA ba...</td>\n",
       "      <td>stupidosaur   vidyut  team  bjp cia baby cctv ...</td>\n",
       "      <td>stupidosaur vidyut team bjp cia baby cctv evm ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[stupidosaur, vidyut, team, bjp, cia, baby, cc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14939</th>\n",
       "      <td>@Vidyut B team of BJP. CIA baby. CCTV, EVM but...</td>\n",
       "      <td>team  bjp cia baby cctv evm but with vvpat su...</td>\n",
       "      <td>team bjp cia baby cctv evm but with vvpat supp...</td>\n",
       "      <td>0</td>\n",
       "      <td>[team, bjp, cia, baby, cctv, evm, but, with, v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14940 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet   \n",
       "0      RT @rssurjewala: Critical question: Was PayTM ...  \\\n",
       "1      RT @Hemant_80: Did you vote on #Demonetization...   \n",
       "2      RT @roshankar: Former FinSec, RBI Dy Governor,...   \n",
       "3      RT @ANI_news: Gurugram (Haryana): Post office ...   \n",
       "4      RT @satishacharya: Reddy Wedding! @mail_today ...   \n",
       "...                                                  ...   \n",
       "14935  RT @saxenavishakha: Ghost of demonetization re...   \n",
       "14936  N d modi fans-d true nationalists of the count...   \n",
       "14937  RT @bharat_builder: Lol. Demonetization has fi...   \n",
       "14938  RT @Stupidosaur: @Vidyut B team of BJP. CIA ba...   \n",
       "14939  @Vidyut B team of BJP. CIA baby. CCTV, EVM but...   \n",
       "\n",
       "                                           cleaned_tweet   \n",
       "0      rssurjewala  critical question  was paytm info...  \\\n",
       "1      hemant 80  did you vote   demonetization  modi...   \n",
       "2      roshankar  former finsec rbi  governor cbdt ch...   \n",
       "3      ani news  gurugram haryana  post office employ...   \n",
       "4      satishacharya  reddy wedding   mail today cart...   \n",
       "...                                                  ...   \n",
       "14935  saxenavishakha  ghost  demonetization returns ...   \n",
       "14936   modi fansd true nationalists  the country sti...   \n",
       "14937  bharat builder  lol demonetization has fixed  ...   \n",
       "14938  stupidosaur   vidyut  team  bjp cia baby cctv ...   \n",
       "14939   team  bjp cia baby cctv evm but with vvpat su...   \n",
       "\n",
       "                                     fully_cleaned_tweet  sentiment   \n",
       "0      rssurjewala critical question was paytm inform...          1  \\\n",
       "1      hemant 80 did you vote demonetization modi sur...          0   \n",
       "2      roshankar former finsec rbi governor cbdt chai...          0   \n",
       "3      ani news gurugram haryana post office employee...          0   \n",
       "4      satishacharya reddy wedding mail today cartoon...          0   \n",
       "...                                                  ...        ...   \n",
       "14935  saxenavishakha ghost demonetization returns wi...          0   \n",
       "14936  modi fansd true nationalists the country stil ...          0   \n",
       "14937  bharat builder lol demonetization has fixed lo...          1   \n",
       "14938  stupidosaur vidyut team bjp cia baby cctv evm ...          0   \n",
       "14939  team bjp cia baby cctv evm but with vvpat supp...          0   \n",
       "\n",
       "                                         tokenized_tweet  cl_num  \n",
       "0      [rssurjewala, critical, question, was, paytm, ...       1  \n",
       "1      [hemant, did, you, vote, demonetization, modi,...       5  \n",
       "2      [roshankar, former, finsec, rbi, governor, cbd...       3  \n",
       "3      [ani, news, gurugram, haryana, post, office, e...       0  \n",
       "4      [satishacharya, reddy, wedding, mail, today, c...       0  \n",
       "...                                                  ...     ...  \n",
       "14935  [saxenavishakha, ghost, demonetization, return...       3  \n",
       "14936  [modi, fansd, true, nationalists, the, country...       9  \n",
       "14937  [bharat, builder, lol, demonetization, has, fi...       6  \n",
       "14938  [stupidosaur, vidyut, team, bjp, cia, baby, cc...       0  \n",
       "14939  [team, bjp, cia, baby, cctv, evm, but, with, v...       0  \n",
       "\n",
       "[14940 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOrdered = pd.DataFrame(df)\n",
    "\n",
    "#Compute how many times a tweet has been 'retweeted' - that is, how many rows in dfOrdered are identical\n",
    "dfOrdered['tokenized_tweet'] = dfOrdered['tokenized_tweet'].apply(tuple)\n",
    "dfUnique = dfOrdered.groupby(['tweet', 'cleaned_tweet', 'fully_cleaned_tweet', 'sentiment','tokenized_tweet', 'cl_num']).size().reset_index(name=\"freq\")\n",
    "dfUnique = dfUnique.sort_values(by=['cl_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUnique['tokenized_tweet'] = dfUnique['tokenized_tweet'].apply(list)\n",
    "dfOrdered['tokenized_tweet'] = dfOrdered['tokenized_tweet'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>fully_cleaned_tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>cl_num</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>India's experiencing a mobile POS boom followi...</td>\n",
       "      <td>indias experiencing  mobile pos boom following...</td>\n",
       "      <td>indias experiencing mobile pos boom following ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[indias, experiencing, mobile, pos, boom, foll...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>India's attempt to go cashless is turning food...</td>\n",
       "      <td>indias attempt   cashless  turning food vouche...</td>\n",
       "      <td>indias attempt cashless turning food vouchers ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[indias, attempt, cashless, turning, food, vou...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5145</th>\n",
       "      <td>…and modi ji’s Finance Minister Jaitley Lies t...</td>\n",
       "      <td>and modi   finance minister jaitley lies  the...</td>\n",
       "      <td>and modi finance minister jaitley lies the cor...</td>\n",
       "      <td>0</td>\n",
       "      <td>[and, modi, finance, minister, jaitley, lies, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2574</th>\n",
       "      <td>India's exports grew fastest in 2016-17 in las...</td>\n",
       "      <td>indias exports grew fastest  201617  last 5 ye...</td>\n",
       "      <td>indias exports grew fastest 201617 last 5 year...</td>\n",
       "      <td>0</td>\n",
       "      <td>[indias, exports, grew, fastest, last, years, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>India's exports grow fastest in last 5 years a...</td>\n",
       "      <td>indias exports grow fastest  last 5 years and ...</td>\n",
       "      <td>indias exports grow fastest last 5 years and y...</td>\n",
       "      <td>0</td>\n",
       "      <td>[indias, exports, grow, fastest, last, years, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>RT @indu_jha: Where are those dogs who started...</td>\n",
       "      <td>indu jha  where are those dogs who started bar...</td>\n",
       "      <td>indu jha where are those dogs who started bark...</td>\n",
       "      <td>0</td>\n",
       "      <td>[indu, jha, where, are, those, dogs, who, star...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>Bhaichung Bhutia protest against #Demonetizati...</td>\n",
       "      <td>bhaichung bhutia protest against  demonetizati...</td>\n",
       "      <td>bhaichung bhutia protest against demonetizatio...</td>\n",
       "      <td>0</td>\n",
       "      <td>[bhaichung, bhutia, protest, against, demoneti...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>(Table shows)Demonetization had least effect (...</td>\n",
       "      <td>table showsdemonetization had least effect  re...</td>\n",
       "      <td>table showsdemonetization had least effect red...</td>\n",
       "      <td>0</td>\n",
       "      <td>[table, showsdemonetization, had, least, effec...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3483</th>\n",
       "      <td>RT @ShirishKunder: 500 Crore wedding of Janard...</td>\n",
       "      <td>shirishkunder  500 crore wedding  janardhan re...</td>\n",
       "      <td>shirishkunder 500 crore wedding janardhan redd...</td>\n",
       "      <td>0</td>\n",
       "      <td>[shirishkunder, crore, wedding, janardhan, red...</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>#DeMonetization - Some thoughts - 2 weeks over...</td>\n",
       "      <td>demonetization  some thoughts  2 weeks over a...</td>\n",
       "      <td>demonetization some thoughts 2 weeks over afte...</td>\n",
       "      <td>0</td>\n",
       "      <td>[demonetization, some, thoughts, weeks, over, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5147 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet   \n",
       "2573  India's experiencing a mobile POS boom followi...  \\\n",
       "2568  India's attempt to go cashless is turning food...   \n",
       "5145  …and modi ji’s Finance Minister Jaitley Lies t...   \n",
       "2574  India's exports grew fastest in 2016-17 in las...   \n",
       "2575  India's exports grow fastest in last 5 years a...   \n",
       "...                                                 ...   \n",
       "3909  RT @indu_jha: Where are those dogs who started...   \n",
       "1828  Bhaichung Bhutia protest against #Demonetizati...   \n",
       "506   (Table shows)Demonetization had least effect (...   \n",
       "3483  RT @ShirishKunder: 500 Crore wedding of Janard...   \n",
       "77    #DeMonetization - Some thoughts - 2 weeks over...   \n",
       "\n",
       "                                          cleaned_tweet   \n",
       "2573  indias experiencing  mobile pos boom following...  \\\n",
       "2568  indias attempt   cashless  turning food vouche...   \n",
       "5145   and modi   finance minister jaitley lies  the...   \n",
       "2574  indias exports grew fastest  201617  last 5 ye...   \n",
       "2575  indias exports grow fastest  last 5 years and ...   \n",
       "...                                                 ...   \n",
       "3909  indu jha  where are those dogs who started bar...   \n",
       "1828  bhaichung bhutia protest against  demonetizati...   \n",
       "506   table showsdemonetization had least effect  re...   \n",
       "3483  shirishkunder  500 crore wedding  janardhan re...   \n",
       "77     demonetization  some thoughts  2 weeks over a...   \n",
       "\n",
       "                                    fully_cleaned_tweet  sentiment   \n",
       "2573  indias experiencing mobile pos boom following ...          0  \\\n",
       "2568  indias attempt cashless turning food vouchers ...          0   \n",
       "5145  and modi finance minister jaitley lies the cor...          0   \n",
       "2574  indias exports grew fastest 201617 last 5 year...          0   \n",
       "2575  indias exports grow fastest last 5 years and y...          0   \n",
       "...                                                 ...        ...   \n",
       "3909  indu jha where are those dogs who started bark...          0   \n",
       "1828  bhaichung bhutia protest against demonetizatio...          0   \n",
       "506   table showsdemonetization had least effect red...          0   \n",
       "3483  shirishkunder 500 crore wedding janardhan redd...          0   \n",
       "77    demonetization some thoughts 2 weeks over afte...          0   \n",
       "\n",
       "                                        tokenized_tweet  cl_num  freq  \n",
       "2573  [indias, experiencing, mobile, pos, boom, foll...       0     1  \n",
       "2568  [indias, attempt, cashless, turning, food, vou...       0     1  \n",
       "5145  [and, modi, finance, minister, jaitley, lies, ...       0     1  \n",
       "2574  [indias, exports, grew, fastest, last, years, ...       0     1  \n",
       "2575  [indias, exports, grow, fastest, last, years, ...       0     1  \n",
       "...                                                 ...     ...   ...  \n",
       "3909  [indu, jha, where, are, those, dogs, who, star...      10     1  \n",
       "1828  [bhaichung, bhutia, protest, against, demoneti...      10     1  \n",
       "506   [table, showsdemonetization, had, least, effec...      10     1  \n",
       "3483  [shirishkunder, crore, wedding, janardhan, red...      10     9  \n",
       "77    [demonetization, some, thoughts, weeks, over, ...      10     1  \n",
       "\n",
       "[5147 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfUnique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discard the clusters with poor Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 : 0.3286860500156128\n",
      "Cluster 1 : 0.6915130692335203\n",
      "Cluster 2 : 0.20232296695245144\n",
      "Cluster 3 : 0.5270271683340432\n",
      "Cluster 4 : 0.6838468218033299\n",
      "Cluster 5 : 0.48881357777120044\n",
      "Cluster 6 : 0.5420609738388532\n",
      "Cluster 7 : 0.8716226396845148\n",
      "Cluster 8 : 0.5101253470970537\n",
      "Cluster 9 : 0.4920340611886833\n",
      "Cluster 10 : 0.6486355697282266\n"
     ]
    }
   ],
   "source": [
    "# Compute the silhouette scores for each sample\n",
    "sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "poor_cluster_indices = []\n",
    "avg_cluster_sil_score = []\n",
    "\n",
    "for i in range(n_best_clusters):\n",
    "# Aggregate the silhouette scores for samples belonging to\n",
    "# cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        avgscore = (np.mean(ith_cluster_silhouette_values))   #average silhouette score for each cluster\n",
    "        avg_cluster_sil_score = np.append(avg_cluster_sil_score, avgscore)\n",
    "        print('Cluster',i, ':', avgscore)\n",
    "        if avgscore < 0.2:\n",
    "            poor_cluster_indices = np.append(poor_cluster_indices, i)\n",
    "            \n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poor_cluster_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove those rows where cluster value match poor_cluster_indices \n",
    "avg_cluster_sil_score_final = []\n",
    "cluster_name = np.unique(dfOrdered['cl_num'])\n",
    "\n",
    "if (len(poor_cluster_indices)!=0):\n",
    "    n_final_clusters = n_best_clusters - len(poor_cluster_indices)\n",
    "    for i in poor_cluster_indices:\n",
    "        dfUnique = dfUnique[dfUnique['cl_num'] != i]\n",
    "    for j in cluster_name:\n",
    "        if j not in poor_cluster_indices:    \n",
    "            avg_cluster_sil_score_final = np.append(avg_cluster_sil_score_final, avg_cluster_sil_score[j])\n",
    "            \n",
    "    cluster_name = np.unique(dfUnique['cl_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUnique['cl_num'] = abs(dfUnique['cl_num'])\n",
    "dfUnique = dfUnique.sort_values(by=['cl_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>fully_cleaned_tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>cl_num</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>India's experiencing a mobile POS boom followi...</td>\n",
       "      <td>indias experiencing  mobile pos boom following...</td>\n",
       "      <td>indias experiencing mobile pos boom following ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[indias, experiencing, mobile, pos, boom, foll...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4109</th>\n",
       "      <td>RT @ram2sun: Don't give ideas to India's wanna...</td>\n",
       "      <td>ram2sun  dont give ideas  indias wannabe mugab...</td>\n",
       "      <td>ram2sun dont give ideas indias wannabe mugabe ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[dont, give, ideas, indias, wannabe, mugabe, d...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>RT @priyankac19: Shri Modiji if you have jokes...</td>\n",
       "      <td>priyankac19  shri modiji  you have jokes  crac...</td>\n",
       "      <td>priyankac19 shri modiji you have jokes crack d...</td>\n",
       "      <td>0</td>\n",
       "      <td>[shri, modiji, you, have, jokes, crack, demone...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4108</th>\n",
       "      <td>RT @rajeev_mp: Today again #demonetization deb...</td>\n",
       "      <td>rajeev   today again  demonetization debate  p...</td>\n",
       "      <td>rajeev today again demonetization debate parlm...</td>\n",
       "      <td>0</td>\n",
       "      <td>[rajeev, today, again, demonetization, debate,...</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4107</th>\n",
       "      <td>RT @rajeev_mp: Today a washout of #parliament....</td>\n",
       "      <td>rajeev   today  washout   parliament  cudnt sp...</td>\n",
       "      <td>rajeev today washout parliament cudnt spk bcoz...</td>\n",
       "      <td>0</td>\n",
       "      <td>[rajeev, today, washout, parliament, cudnt, sp...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>A maligned perspective by @mihirssharma must b...</td>\n",
       "      <td>maligned perspective   mihirssharma must  stu...</td>\n",
       "      <td>maligned perspective mihirssharma must stung d...</td>\n",
       "      <td>0</td>\n",
       "      <td>[maligned, perspective, mihirssharma, must, st...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>#Demonetization Cash Crunch in #Andaman Banks ...</td>\n",
       "      <td>demonetization cash crunch   andaman banks ca...</td>\n",
       "      <td>demonetization cash crunch andaman banks causi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[demonetization, cash, crunch, andaman, banks,...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3286</th>\n",
       "      <td>RT @LivingOnChi: 1/21/17 Demonetization: The S...</td>\n",
       "      <td>livingonchi  12117 demonetization  the siniste...</td>\n",
       "      <td>livingonchi 12117 demonetization the sinister ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[livingonchi, demonetization, the, sinister, a...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3284</th>\n",
       "      <td>RT @LivingOnChi: .@zerohedge 1/21/17 Demonetiz...</td>\n",
       "      <td>livingonchi   zerohedge 12117 demonetization  ...</td>\n",
       "      <td>livingonchi zerohedge 12117 demonetization the...</td>\n",
       "      <td>0</td>\n",
       "      <td>[livingonchi, zerohedge, demonetization, the, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>#DeMonetization - Some thoughts - 2 weeks over...</td>\n",
       "      <td>demonetization  some thoughts  2 weeks over a...</td>\n",
       "      <td>demonetization some thoughts 2 weeks over afte...</td>\n",
       "      <td>0</td>\n",
       "      <td>[demonetization, some, thoughts, weeks, over, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5147 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet   \n",
       "2573  India's experiencing a mobile POS boom followi...  \\\n",
       "4109  RT @ram2sun: Don't give ideas to India's wanna...   \n",
       "4092  RT @priyankac19: Shri Modiji if you have jokes...   \n",
       "4108  RT @rajeev_mp: Today again #demonetization deb...   \n",
       "4107  RT @rajeev_mp: Today a washout of #parliament....   \n",
       "...                                                 ...   \n",
       "1646  A maligned perspective by @mihirssharma must b...   \n",
       "142   #Demonetization Cash Crunch in #Andaman Banks ...   \n",
       "3286  RT @LivingOnChi: 1/21/17 Demonetization: The S...   \n",
       "3284  RT @LivingOnChi: .@zerohedge 1/21/17 Demonetiz...   \n",
       "77    #DeMonetization - Some thoughts - 2 weeks over...   \n",
       "\n",
       "                                          cleaned_tweet   \n",
       "2573  indias experiencing  mobile pos boom following...  \\\n",
       "4109  ram2sun  dont give ideas  indias wannabe mugab...   \n",
       "4092  priyankac19  shri modiji  you have jokes  crac...   \n",
       "4108  rajeev   today again  demonetization debate  p...   \n",
       "4107  rajeev   today  washout   parliament  cudnt sp...   \n",
       "...                                                 ...   \n",
       "1646   maligned perspective   mihirssharma must  stu...   \n",
       "142    demonetization cash crunch   andaman banks ca...   \n",
       "3286  livingonchi  12117 demonetization  the siniste...   \n",
       "3284  livingonchi   zerohedge 12117 demonetization  ...   \n",
       "77     demonetization  some thoughts  2 weeks over a...   \n",
       "\n",
       "                                    fully_cleaned_tweet  sentiment   \n",
       "2573  indias experiencing mobile pos boom following ...          0  \\\n",
       "4109  ram2sun dont give ideas indias wannabe mugabe ...          0   \n",
       "4092  priyankac19 shri modiji you have jokes crack d...          0   \n",
       "4108  rajeev today again demonetization debate parlm...          0   \n",
       "4107  rajeev today washout parliament cudnt spk bcoz...          0   \n",
       "...                                                 ...        ...   \n",
       "1646  maligned perspective mihirssharma must stung d...          0   \n",
       "142   demonetization cash crunch andaman banks causi...          0   \n",
       "3286  livingonchi 12117 demonetization the sinister ...          0   \n",
       "3284  livingonchi zerohedge 12117 demonetization the...          0   \n",
       "77    demonetization some thoughts 2 weeks over afte...          0   \n",
       "\n",
       "                                        tokenized_tweet  cl_num  freq  \n",
       "2573  [indias, experiencing, mobile, pos, boom, foll...       0     1  \n",
       "4109  [dont, give, ideas, indias, wannabe, mugabe, d...       0     5  \n",
       "4092  [shri, modiji, you, have, jokes, crack, demone...       0     1  \n",
       "4108  [rajeev, today, again, demonetization, debate,...       0    20  \n",
       "4107  [rajeev, today, washout, parliament, cudnt, sp...       0     1  \n",
       "...                                                 ...     ...   ...  \n",
       "1646  [maligned, perspective, mihirssharma, must, st...      10     1  \n",
       "142   [demonetization, cash, crunch, andaman, banks,...      10     1  \n",
       "3286  [livingonchi, demonetization, the, sinister, a...      10     1  \n",
       "3284  [livingonchi, zerohedge, demonetization, the, ...      10     3  \n",
       "77    [demonetization, some, thoughts, weeks, over, ...      10     1  \n",
       "\n",
       "[5147 rows x 7 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfUnique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Calculate abstraction and expression for each narrative \n",
    "Note that each cluster represents a narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_to_consider = 'fully_cleaned_tweet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "final_clusters = np.unique(dfUnique['cl_num'])\n",
    "print(final_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store all tweets corresponding to each cluster in a file\n",
    "for i in final_clusters:\n",
    "    with open('./tweets_Cluster_'+str(i)+'.txt','w') as out:\n",
    "        y = ''\n",
    "        for x in dfUnique[tweets_to_consider][dfUnique.cl_num == i]:    \n",
    "            y = y + x + '. '\n",
    "        out.write(y)\n",
    "        out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            extracted_phrases  cluster_num\n",
      "0              boom following          0.0\n",
      "1                  mobile pos          0.0\n",
      "2                    pos boom          0.0\n",
      "3          pos boom following          0.0\n",
      "4      dont give ideas indias          0.0\n",
      "...                       ...          ...\n",
      "15452                 2 weeks         10.0\n",
      "15453          demonetization         10.0\n",
      "15454           lines reduced         10.0\n",
      "15455    speech lines reduced         10.0\n",
      "15456        thoughts 2 weeks         10.0\n",
      "\n",
      "[15457 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#A combination of (Noun, adjective, cardinal number, foreign word and Verb) are being extracted now\n",
    "#Extract chunks matching pattern. Patterns are:\n",
    "#1) Noun phrase (2 or more nouns occurring together. Ex United states of America, Abdul Kalam etc)\n",
    "#2) Number followed by Noun (Ex: 28 Terrorists, 45th President)\n",
    "#3) Adjective followed by Noun (Ex: Economic impact, beautiful inauguration)\n",
    "#4) Foreign word (Ex: Jallikattu, Narendra modi, Pappu)\n",
    "#5) Noun followed by Verb (Ex: Terrorists arrested)\n",
    "#And a combination of all 5\n",
    "        \n",
    "import re\n",
    "import nltk\n",
    "\n",
    "phrases = pd.DataFrame({'extracted_phrases': [], 'cluster_num': []})\n",
    "\n",
    "\n",
    "A = '(CD|JJ)/\\w+\\s'  #cd or jj\n",
    "B = '(NN|NNS|NNP|NNPS)/\\w+\\s'  #nouns\n",
    "C = '(VB|VBD|VBG|VBN|VBP|VBZ)/\\w+\\s' #verbs\n",
    "D = 'FW/\\w+\\s'  #foreign word\n",
    "patterns = ['('+A+B+')+', '('+D+B+')+','('+D+')+', '('+B+')+', '('+D+A+B+')+', \n",
    "           '('+B+C+')+', '('+D+B+C+')+', '('+B+A+B+')+', '('+B+B+C+')+'] \n",
    "\n",
    "\n",
    "def extract_phrases(tag1, tag2, sentences):\n",
    "    extract_phrase = []\n",
    "    for sentence in sentences:\n",
    "        phrase = []\n",
    "        next_word = 0\n",
    "        for word, pos in nltk.pos_tag(nltk.word_tokenize(sentence)):\n",
    "            if next_word == 1:\n",
    "                next_word = 0\n",
    "                if pos == tag2:\n",
    "                    extract_phrase = np.append(extract_phrase,phrase + ' ' + word) \n",
    "            \n",
    "            if pos == tag1:\n",
    "                next_word = 1\n",
    "                phrase = word\n",
    "    return extract_phrase\n",
    "\n",
    "for i in cluster_name:\n",
    "    File = open('./tweets_Cluster_'+str(i)+'.txt', 'r') #open file\n",
    "    lines = File.read() #read all lines\n",
    "    sentences = nltk.sent_tokenize(lines) #tokenize sentences\n",
    "\n",
    "    for sentence in sentences: \n",
    "        f = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "        tag_seq = []\n",
    "        for word, pos in f:\n",
    "            tag_seq.append(pos+'/'+ word)\n",
    "        X = \" \".join(tag_seq)\n",
    "\n",
    "        phrase = []\n",
    "        for j in range(len(patterns)):\n",
    "            if re.search(patterns[j], X):\n",
    "                phrase.append(' '.join([word.split('/')[1] for word in re.search(patterns[j], X).group(0).split()]))\n",
    "    \n",
    "        k = pd.DataFrame({'extracted_phrases': np.unique(phrase), 'cluster_num': int(i)})\n",
    "    \n",
    "        phrases = pd.concat([phrases,k], ignore_index = True)\n",
    "\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping the largest phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each phrase identified replace all the substrings by the largest phrase \n",
    "#Ex: lakh looted,40 lakh looted and Rs 40 lakh looted, replace all by single largest phrase - Rs 40 lakh looted \n",
    "#i.e. instead of 3 different phrases, there will be only one large phrase\n",
    "\n",
    "phrases_final = pd.DataFrame({'extracted_phrases': [], 'cluster_num': []})\n",
    "for i in cluster_name:\n",
    "    phrases_for_each_cluster = []\n",
    "    cluster_phrases = phrases['extracted_phrases'][phrases.cluster_num == i]\n",
    "    cluster_phrases = np.unique(np.array(cluster_phrases))\n",
    "    for j in range(len(cluster_phrases)):\n",
    "        \n",
    "        phrase = cluster_phrases[j]\n",
    "        updated_cluster_phrases = np.delete((cluster_phrases), j)\n",
    "        if any(phrase in phr for phr in updated_cluster_phrases): \n",
    "            'y'\n",
    "        else: \n",
    "            #considering phrases of length greater than 1 only\n",
    "            if (len(phrase.split(' '))) > 1:\n",
    "                phrases_for_each_cluster.append(phrase)\n",
    "    k = pd.DataFrame({'extracted_phrases': phrases_for_each_cluster, 'cluster_num': int(i) })\n",
    "    \n",
    "    phrases_final = pd.concat([phrases_final,k], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extracted_phrases</th>\n",
       "      <th>cluster_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>007 economictimes mamataofficial mom</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01 deposits indias banks</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05012017but till</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06 bharat</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 end black money 2 end</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7322</th>\n",
       "      <td>withdraw cash</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7323</th>\n",
       "      <td>withdraw money</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7324</th>\n",
       "      <td>withdrawal restrictions were</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7325</th>\n",
       "      <td>withdrawals allowed</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7326</th>\n",
       "      <td>yesterday 5 atms</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7327 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         extracted_phrases  cluster_num\n",
       "0     007 economictimes mamataofficial mom          0.0\n",
       "1                 01 deposits indias banks          0.0\n",
       "2                         05012017but till          0.0\n",
       "3                                06 bharat          0.0\n",
       "4                  1 end black money 2 end          0.0\n",
       "...                                    ...          ...\n",
       "7322                         withdraw cash         10.0\n",
       "7323                        withdraw money         10.0\n",
       "7324          withdrawal restrictions were         10.0\n",
       "7325                   withdrawals allowed         10.0\n",
       "7326                      yesterday 5 atms         10.0\n",
       "\n",
       "[7327 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['extracted_phrases', 'cluster_num'], dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each phrase in each cluster, calculate term frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term-frequency : For each cluster, calculate the number of times a given phrase occur in the tweets of that cluster\n",
    "\n",
    "phrases_final['term_freq'] = len(phrases_final)*[0]\n",
    "\n",
    "for i in cluster_name:\n",
    "    for phrase in phrases_final['extracted_phrases'][phrases_final.cluster_num == i]:\n",
    "        tweets = dfUnique[tweets_to_consider][dfUnique.cl_num == i]\n",
    "        for tweet in tweets:\n",
    "            if phrase in tweet:\n",
    "                phrases_final['term_freq'][(phrases_final.extracted_phrases == phrase) & (phrases_final.cluster_num == i)] = phrases_final['term_freq'][(phrases_final.extracted_phrases == phrase) & (phrases_final.cluster_num == i)] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each phrase in each cluster, calculate document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document-frequency\n",
    "phrases_final['doc_freq'] = len(phrases_final)*[0]\n",
    "\n",
    "\n",
    "# for each phrase, compute the number of clusters that Sphrase occurs in\n",
    "for phrase in phrases_final['extracted_phrases']:\n",
    "    for i in cluster_name:\n",
    "        all_tweets = ''\n",
    "        for tweet in dfUnique[tweets_to_consider][dfUnique.cl_num == i]:\n",
    "            all_tweets = all_tweets + tweet + '. ' \n",
    "        if phrase in all_tweets:\n",
    "            phrases_final['doc_freq'][(phrases_final.extracted_phrases == phrase) & (phrases_final.cluster_num == i)] = phrases_final['doc_freq'][(phrases_final.extracted_phrases == phrase) & (phrases_final.cluster_num == i)] + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "phrases_final['doc_freq'] = phrases_final['doc_freq'].apply(lambda x: math.log10(n_best_clusters/(x)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each phrase in each cluster, calculate tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_final['tf-idf'] = phrases_final['term_freq']*phrases_final['doc_freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extracted_phrases</th>\n",
       "      <th>cluster_num</th>\n",
       "      <th>term_freq</th>\n",
       "      <th>doc_freq</th>\n",
       "      <th>tf-idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>007 economictimes mamataofficial mom</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041393</td>\n",
       "      <td>1.041393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01 deposits indias banks</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041393</td>\n",
       "      <td>1.041393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05012017but till</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041393</td>\n",
       "      <td>1.041393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06 bharat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041393</td>\n",
       "      <td>1.041393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 end black money 2 end</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.041393</td>\n",
       "      <td>2.082785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7322</th>\n",
       "      <td>withdraw cash</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.564271</td>\n",
       "      <td>1.128543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7323</th>\n",
       "      <td>withdraw money</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.740363</td>\n",
       "      <td>0.740363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7324</th>\n",
       "      <td>withdrawal restrictions were</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.740363</td>\n",
       "      <td>0.740363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7325</th>\n",
       "      <td>withdrawals allowed</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041393</td>\n",
       "      <td>1.041393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7326</th>\n",
       "      <td>yesterday 5 atms</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.041393</td>\n",
       "      <td>1.041393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7327 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         extracted_phrases  cluster_num  term_freq  doc_freq   \n",
       "0     007 economictimes mamataofficial mom          0.0          1  1.041393  \\\n",
       "1                 01 deposits indias banks          0.0          1  1.041393   \n",
       "2                         05012017but till          0.0          1  1.041393   \n",
       "3                                06 bharat          0.0          1  1.041393   \n",
       "4                  1 end black money 2 end          0.0          2  1.041393   \n",
       "...                                    ...          ...        ...       ...   \n",
       "7322                         withdraw cash         10.0          2  0.564271   \n",
       "7323                        withdraw money         10.0          1  0.740363   \n",
       "7324          withdrawal restrictions were         10.0          1  0.740363   \n",
       "7325                   withdrawals allowed         10.0          1  1.041393   \n",
       "7326                      yesterday 5 atms         10.0          1  1.041393   \n",
       "\n",
       "        tf-idf  \n",
       "0     1.041393  \n",
       "1     1.041393  \n",
       "2     1.041393  \n",
       "3     1.041393  \n",
       "4     2.082785  \n",
       "...        ...  \n",
       "7322  1.128543  \n",
       "7323  0.740363  \n",
       "7324  0.740363  \n",
       "7325  1.041393  \n",
       "7326  1.041393  \n",
       "\n",
       "[7327 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each cluster find top few phrases and respective sentiment\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_final['diff_tf-idf'] = len(phrases_final)*[0]\n",
    "\n",
    "narrative = pd.DataFrame({'cl_num': [], 'abstraction': []})\n",
    "for i in cluster_name: \n",
    "    # arrange in descending order of tf-idf score\n",
    "    phrases_final = phrases_final.sort_values(['cluster_num','tf-idf'], ascending=[1,0])\n",
    "    \n",
    "    #Break this distribution at a point where the difference between any consecutive phrases is maximum\n",
    "    #difference between consecutive values of tf-idf \n",
    "    phrases_final['diff_tf-idf'][phrases_final.cluster_num == i] = abs(phrases_final['tf-idf'][phrases_final.cluster_num == i] - phrases_final['tf-idf'][phrases_final.cluster_num == i].shift(1))\n",
    "\n",
    "    #The last value for each cluster will be 'NaN'. Replacing it with '0'. \n",
    "    phrases_final = phrases_final.fillna(0)\n",
    "    \n",
    "    phrases_final = phrases_final.reset_index(drop = True) #to avoid old index being added as a new column\n",
    "    if len(phrases_final[phrases_final.cluster_num == i]) != 0:\n",
    "        \n",
    "        #index corresponding to the highest difference\n",
    " \n",
    "        ind = (phrases_final['diff_tf-idf'][phrases_final.cluster_num == i]).idxmax()\n",
    "        \n",
    "        abstract = phrases_final['extracted_phrases'][:ind+1][phrases_final.cluster_num == i]\n",
    "    \n",
    "    \n",
    "        #store the abstraction corresponding to each cluster\n",
    "        k = pd.DataFrame({'cl_num': int(i), 'abstraction': abstract})\n",
    "        narrative = pd.concat([narrative,k], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>fully_cleaned_tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>cl_num</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>India's experiencing a mobile POS boom followi...</td>\n",
       "      <td>indias experiencing  mobile pos boom following...</td>\n",
       "      <td>indias experiencing mobile pos boom following ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[indias, experiencing, mobile, pos, boom, foll...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4109</th>\n",
       "      <td>RT @ram2sun: Don't give ideas to India's wanna...</td>\n",
       "      <td>ram2sun  dont give ideas  indias wannabe mugab...</td>\n",
       "      <td>ram2sun dont give ideas indias wannabe mugabe ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[dont, give, ideas, indias, wannabe, mugabe, d...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>RT @priyankac19: Shri Modiji if you have jokes...</td>\n",
       "      <td>priyankac19  shri modiji  you have jokes  crac...</td>\n",
       "      <td>priyankac19 shri modiji you have jokes crack d...</td>\n",
       "      <td>0</td>\n",
       "      <td>[shri, modiji, you, have, jokes, crack, demone...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4108</th>\n",
       "      <td>RT @rajeev_mp: Today again #demonetization deb...</td>\n",
       "      <td>rajeev   today again  demonetization debate  p...</td>\n",
       "      <td>rajeev today again demonetization debate parlm...</td>\n",
       "      <td>0</td>\n",
       "      <td>[rajeev, today, again, demonetization, debate,...</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4107</th>\n",
       "      <td>RT @rajeev_mp: Today a washout of #parliament....</td>\n",
       "      <td>rajeev   today  washout   parliament  cudnt sp...</td>\n",
       "      <td>rajeev today washout parliament cudnt spk bcoz...</td>\n",
       "      <td>0</td>\n",
       "      <td>[rajeev, today, washout, parliament, cudnt, sp...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>A maligned perspective by @mihirssharma must b...</td>\n",
       "      <td>maligned perspective   mihirssharma must  stu...</td>\n",
       "      <td>maligned perspective mihirssharma must stung d...</td>\n",
       "      <td>0</td>\n",
       "      <td>[maligned, perspective, mihirssharma, must, st...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>#Demonetization Cash Crunch in #Andaman Banks ...</td>\n",
       "      <td>demonetization cash crunch   andaman banks ca...</td>\n",
       "      <td>demonetization cash crunch andaman banks causi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[demonetization, cash, crunch, andaman, banks,...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3286</th>\n",
       "      <td>RT @LivingOnChi: 1/21/17 Demonetization: The S...</td>\n",
       "      <td>livingonchi  12117 demonetization  the siniste...</td>\n",
       "      <td>livingonchi 12117 demonetization the sinister ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[livingonchi, demonetization, the, sinister, a...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3284</th>\n",
       "      <td>RT @LivingOnChi: .@zerohedge 1/21/17 Demonetiz...</td>\n",
       "      <td>livingonchi   zerohedge 12117 demonetization  ...</td>\n",
       "      <td>livingonchi zerohedge 12117 demonetization the...</td>\n",
       "      <td>0</td>\n",
       "      <td>[livingonchi, zerohedge, demonetization, the, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>#DeMonetization - Some thoughts - 2 weeks over...</td>\n",
       "      <td>demonetization  some thoughts  2 weeks over a...</td>\n",
       "      <td>demonetization some thoughts 2 weeks over afte...</td>\n",
       "      <td>0</td>\n",
       "      <td>[demonetization, some, thoughts, weeks, over, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5147 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet   \n",
       "2573  India's experiencing a mobile POS boom followi...  \\\n",
       "4109  RT @ram2sun: Don't give ideas to India's wanna...   \n",
       "4092  RT @priyankac19: Shri Modiji if you have jokes...   \n",
       "4108  RT @rajeev_mp: Today again #demonetization deb...   \n",
       "4107  RT @rajeev_mp: Today a washout of #parliament....   \n",
       "...                                                 ...   \n",
       "1646  A maligned perspective by @mihirssharma must b...   \n",
       "142   #Demonetization Cash Crunch in #Andaman Banks ...   \n",
       "3286  RT @LivingOnChi: 1/21/17 Demonetization: The S...   \n",
       "3284  RT @LivingOnChi: .@zerohedge 1/21/17 Demonetiz...   \n",
       "77    #DeMonetization - Some thoughts - 2 weeks over...   \n",
       "\n",
       "                                          cleaned_tweet   \n",
       "2573  indias experiencing  mobile pos boom following...  \\\n",
       "4109  ram2sun  dont give ideas  indias wannabe mugab...   \n",
       "4092  priyankac19  shri modiji  you have jokes  crac...   \n",
       "4108  rajeev   today again  demonetization debate  p...   \n",
       "4107  rajeev   today  washout   parliament  cudnt sp...   \n",
       "...                                                 ...   \n",
       "1646   maligned perspective   mihirssharma must  stu...   \n",
       "142    demonetization cash crunch   andaman banks ca...   \n",
       "3286  livingonchi  12117 demonetization  the siniste...   \n",
       "3284  livingonchi   zerohedge 12117 demonetization  ...   \n",
       "77     demonetization  some thoughts  2 weeks over a...   \n",
       "\n",
       "                                    fully_cleaned_tweet  sentiment   \n",
       "2573  indias experiencing mobile pos boom following ...          0  \\\n",
       "4109  ram2sun dont give ideas indias wannabe mugabe ...          0   \n",
       "4092  priyankac19 shri modiji you have jokes crack d...          0   \n",
       "4108  rajeev today again demonetization debate parlm...          0   \n",
       "4107  rajeev today washout parliament cudnt spk bcoz...          0   \n",
       "...                                                 ...        ...   \n",
       "1646  maligned perspective mihirssharma must stung d...          0   \n",
       "142   demonetization cash crunch andaman banks causi...          0   \n",
       "3286  livingonchi 12117 demonetization the sinister ...          0   \n",
       "3284  livingonchi zerohedge 12117 demonetization the...          0   \n",
       "77    demonetization some thoughts 2 weeks over afte...          0   \n",
       "\n",
       "                                        tokenized_tweet  cl_num  freq  \n",
       "2573  [indias, experiencing, mobile, pos, boom, foll...       0     1  \n",
       "4109  [dont, give, ideas, indias, wannabe, mugabe, d...       0     5  \n",
       "4092  [shri, modiji, you, have, jokes, crack, demone...       0     1  \n",
       "4108  [rajeev, today, again, demonetization, debate,...       0    20  \n",
       "4107  [rajeev, today, washout, parliament, cudnt, sp...       0     1  \n",
       "...                                                 ...     ...   ...  \n",
       "1646  [maligned, perspective, mihirssharma, must, st...      10     1  \n",
       "142   [demonetization, cash, crunch, andaman, banks,...      10     1  \n",
       "3286  [livingonchi, demonetization, the, sinister, a...      10     1  \n",
       "3284  [livingonchi, zerohedge, demonetization, the, ...      10     3  \n",
       "77    [demonetization, some, thoughts, weeks, over, ...      10     1  \n",
       "\n",
       "[5147 rows x 7 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfUnique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning polarity based on the sentiment for each tweet 2=negative, 1=positive, 3=neutral\n",
    "dfUnique['polarity'] = np.NaN\n",
    "dfUnique['polarity'][dfUnique.sentiment == 0.5] = \"3\"\n",
    "dfUnique['polarity'][dfUnique.sentiment == 1] = \"1\"\n",
    "dfUnique['polarity'][dfUnique.sentiment == 0] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign the sentiment to each extracted phrases\n",
    "count the number of tweets, a phrase has occurred in positive, negative and neutral context. Assign the most occurred sentiment to the phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['1']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n",
      "['2']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#find the highest occurring sentiment corresponding to each tweet\n",
    "def find_mode(a):\n",
    "    b = Counter(a).most_common(3)\n",
    "    mode = []; c_max = 0\n",
    "    for a,c in b:\n",
    "        if c>c_max:\n",
    "            c_max = c\n",
    "        if c_max == c:\n",
    "            mode.append(a)  \n",
    "    print(mode)\n",
    "    mode.sort()\n",
    "    print(mode)\n",
    "    \n",
    "    ## if mode is 3&2 i.e. neutral and negative, assign the overall sentiment for that phrase as negative, \n",
    "    ## if mode is 3&1 i.e. neutral and positive, assign the overall sentiment for that phrase as positive,\n",
    "    ## if mode is 2&1 i.e. negative and positive, assign the overall sentiment for that phrase as neutal, \n",
    "    ## if mode is 3&2&1 i.e. negative, positive and neutral, assign the overall sentiment for that phrase as neutral\n",
    "    \n",
    "    if len(mode) == 1:\n",
    "        return mode[0]\n",
    "    \n",
    "    elif (len(mode) == 2) & (mode[1]=='3'):\n",
    "        return mode[0]\n",
    "    else:\n",
    "        return 3\n",
    "    \n",
    "#1=>+ve 2=>-ve 3=>Neutral\n",
    "narrative['expression'] = -1\n",
    "dfUnique = dfUnique.reset_index(drop = True)\n",
    "for i in cluster_name:\n",
    "    tweets = dfUnique[tweets_to_consider][dfUnique.cl_num == i]\n",
    "    abstracts = narrative['abstraction'][narrative.cl_num == i] \n",
    "    for abst in abstracts:\n",
    "        sent = []\n",
    "        for tweet, polarity in zip(dfUnique[tweets_to_consider][dfUnique.cl_num == i], dfUnique['polarity'][dfUnique.cl_num == i]):\n",
    "            if abst in tweet:\n",
    "                sent = np.append(sent, polarity)\n",
    "        \n",
    "        \n",
    "        if len(sent)!=0:\n",
    "            ## if mode is 3&2-2, 3&1-1, 2&1-3, 3&2&1 - 3\n",
    "            senti = find_mode(sent)\n",
    "            if senti == '2':\n",
    "                sent_value = \"Negative\"\n",
    "            elif senti == '1':\n",
    "                sent_value = \"Positive\"\n",
    "            else:\n",
    "                sent_value = \"Neutral\"\n",
    "            narrative['expression'][(narrative.abstraction == abst) & (narrative.cl_num == i)] = sent_value\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Save the narratives in excel file\n",
    " With each sheet in the file representing 1 narrative ( == 1 cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=int32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sudo pip install xlwt\n",
    "#sudo pip3 install openpyxl\n",
    "from pandas import ExcelWriter\n",
    "\n",
    "#Save the narratives in an excel file \n",
    "\n",
    "writer = pd.ExcelWriter('narrative.xlsx')\n",
    "for i in cluster_name:\n",
    "    df1 = pd.DataFrame(dfUnique[['tweet','freq']][dfUnique.cl_num == i]).sort_values(['freq'], ascending = [0])\n",
    "    df1 = pd.DataFrame({'tweet': dfUnique['tweet'][dfUnique.cl_num == i], 'freq': dfUnique['freq'][dfUnique.cl_num == i]}) \n",
    "    df1 = df1.sort_values(['freq'], ascending = [0]) \n",
    "    #print(df1)\n",
    "    df2 = pd.DataFrame({ 'abstraction': narrative['abstraction'][narrative.cl_num == i], 'expression': narrative['expression'][narrative.cl_num == i]})\n",
    "    df3 = pd.DataFrame({'abstraction': (len(df1)-len(df2))*['-'], 'expression': (len(df1)-len(df2))*['-']})\n",
    "\n",
    "    #print(df3)\n",
    "    \n",
    "    #df2 = df2.append(df3)\n",
    "    df2 = pd.concat([df2,df3])\n",
    "\n",
    "    df1 = df1.reset_index(drop=True)\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "    df1['abstraction'] = df2['abstraction']\n",
    "    df1['expression'] = df2['expression']\n",
    "\n",
    "    df1.to_excel(writer,'narrative_cluster'+str(i))\n",
    "\n",
    "writer.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cl_num</th>\n",
       "      <th>abstraction</th>\n",
       "      <th>expression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>attempt cashless turning food vouchers digital</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>demonetization dea secy dasshaktikanta</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>digital payments</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>indias attempt</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>payments growth india</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>demonetization time</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>youtube video</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>people are</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>baseless young turks</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.0</td>\n",
       "      <td>turks get</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.0</td>\n",
       "      <td>youtube demonetization baseless</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>modis demonetization</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.0</td>\n",
       "      <td>demonetization issue</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.0</td>\n",
       "      <td>decision demonetization</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.0</td>\n",
       "      <td>overall impact</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.0</td>\n",
       "      <td>system india article</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.0</td>\n",
       "      <td>financial services</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.0</td>\n",
       "      <td>implement demonetization</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.0</td>\n",
       "      <td>dear evanspiegel</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.0</td>\n",
       "      <td>demonetization india</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6.0</td>\n",
       "      <td>people have</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6.0</td>\n",
       "      <td>delhi opposition</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.0</td>\n",
       "      <td>indian economy</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.0</td>\n",
       "      <td>narendramodi demonetization</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6.0</td>\n",
       "      <td>opposition leaders demonstration</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.0</td>\n",
       "      <td>cash sales</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7.0</td>\n",
       "      <td>pawar uturned demonetization</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>7.0</td>\n",
       "      <td>coop banks</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8.0</td>\n",
       "      <td>demonetization was</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8.0</td>\n",
       "      <td>market expected</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>9.0</td>\n",
       "      <td>partners doing</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>9.0</td>\n",
       "      <td>aap join</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>10.0</td>\n",
       "      <td>india panic mode</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>10.0</td>\n",
       "      <td>mode cash running</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10.0</td>\n",
       "      <td>gov demonetization</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cl_num                                     abstraction expression\n",
       "0      0.0  attempt cashless turning food vouchers digital   Negative\n",
       "1      0.0          demonetization dea secy dasshaktikanta   Negative\n",
       "2      0.0                                digital payments   Negative\n",
       "3      0.0                                  indias attempt   Negative\n",
       "4      0.0                           payments growth india   Negative\n",
       "5      0.0                             demonetization time   Negative\n",
       "6      1.0                                   youtube video   Positive\n",
       "7      1.0                                      people are   Positive\n",
       "8      2.0                            baseless young turks   Positive\n",
       "9      2.0                                       turks get   Positive\n",
       "10     2.0                 youtube demonetization baseless   Positive\n",
       "11     2.0                            modis demonetization   Positive\n",
       "12     3.0                            demonetization issue   Negative\n",
       "13     3.0                         decision demonetization   Negative\n",
       "14     4.0                                  overall impact   Negative\n",
       "15     4.0                            system india article   Negative\n",
       "16     4.0                              financial services   Negative\n",
       "17     5.0                        implement demonetization   Negative\n",
       "18     5.0                                dear evanspiegel   Negative\n",
       "19     5.0                            demonetization india   Negative\n",
       "20     6.0                                     people have   Positive\n",
       "21     6.0                                delhi opposition   Positive\n",
       "22     6.0                                  indian economy   Positive\n",
       "23     6.0                     narendramodi demonetization   Positive\n",
       "24     6.0                opposition leaders demonstration   Positive\n",
       "25     6.0                                      cash sales   Positive\n",
       "26     7.0                    pawar uturned demonetization   Negative\n",
       "27     7.0                                      coop banks   Negative\n",
       "28     8.0                              demonetization was   Negative\n",
       "29     8.0                                 market expected   Negative\n",
       "30     9.0                                  partners doing   Negative\n",
       "31     9.0                                        aap join   Negative\n",
       "32    10.0                                india panic mode   Negative\n",
       "33    10.0                               mode cash running   Negative\n",
       "34    10.0                              gov demonetization   Negative"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
